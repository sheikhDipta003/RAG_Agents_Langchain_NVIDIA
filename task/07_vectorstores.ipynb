{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e",
   "metadata": {
    "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qk4Uw_iSr3Mc",
   "metadata": {
    "id": "Qk4Uw_iSr3Mc"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 7:** Retrieval-Augmented Generation with Vector Stores</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "In the previous notebook, we learned about embedding models and exercised some of their capabilities. We discussed their intended use cases of longer-form document comparison and found ways to use it as a backbone for more custom semantic comparisons. This notebook will progress these ideas toward the retrieval model's intended use case and explore how to build chatbot systems that rely on *vector stores* to automatically save and retrieve information.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Understand how semantic-similarity-backed systems can facilitate easy-to-use retrieval formulations.\n",
    "\n",
    "- Learn how to incorporate retrieval modules into your chat model systems for a retrieval-augmented generation (RAG) pipeline, which can be applied to tasks like document retrieval and conversation memory buffers.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- This notebook does not attempt to incorporate hierarchical reasoning or non-naive RAG (such as planning agents). Consider what modifications would be necessary to make these components work in an LCEL chain.\n",
    "\n",
    "- Consider when it would be best to move your vector store solution into a scalable service and when a GPU will become necessary for optimization.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Notebook Source:**\n",
    "\n",
    "- This notebook is part of a larger [**NVIDIA Deep Learning Institute**](https://www.nvidia.com/en-us/training/) course titled [**Building RAG Agents with LLMs**](https://www.nvidia.com/en-sg/training/instructor-led-workshops/building-rag-agents-with-llms/). If sharing this material, please give credit and link back to the original course.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5XmeiiOWtuxC",
   "metadata": {
    "id": "5XmeiiOWtuxC"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "## ^^ Comment out if you want to see the pip install process\n",
    "\n",
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37fe234-2bdb-4107-8483-efda9aa5e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf",
   "metadata": {
    "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## Part 1: Summary of RAG Workflows\n",
    "\n",
    "This notebook will explore several paradigms and derive reference code to help you approach some of the most common retrieval-augmented workflows. Specifically, the following sections will be covered (with the differences highlighted):\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***Vector Store Workflow for Conversational Exchanges:***\n",
    "- Generate semantic embedding for each new conversation.\n",
    "- Add the message body to a vector store for retrieval.\n",
    "- Query the vector store for relevant messages to fill in the LLM context.\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***Modified Workflow for an Arbitrary Document:***\n",
    "- **Divide the document into chunks and process them into useful messages.**\n",
    "- Generate semantic embedding for each **new document chunk**.\n",
    "- Add the **chunk bodies** to a vector store for retrieval.\n",
    "- Query the vector store for relevant **chunks** to fill in the LLM context.\n",
    "    - ***Optional:* Modify/synthesize results for better LLM results.**\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Extended Workflow for a Directory of Arbitrary Documents:**\n",
    "- Divide **each document** into chunks and process them into useful messages.\n",
    "- Generate semantic embedding for each new document chunk.\n",
    "- Add the chunk bodies to **a scalable vector database for fast retrieval**.\n",
    "    - ***Optional*: Exploit hierarchical or metadata structures for larger systems.**\n",
    "- Query the **vector database** for relevant chunks to fill in the LLM context.\n",
    "    - *Optional:* Modify/synthesize results for better LLM results.\n",
    "\n",
    "<br>\n",
    "\n",
    "Some of the most important terminology surrounding RAG is covered in detail on the [**LlamaIndex Concepts page**](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html), which itself is a great starting point for progressing towards the LlamaIndex loading and retrieving strategy. We highly recommend using it as a reference as you continue with this notebook and advise you to try out LlamaIndex after the course to consider the pros and cons firsthand!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437",
   "metadata": {
    "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437"
   },
   "source": [
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1cFbKbVvLLnFPs3yWCKIuzXkhBWh6nLQY\" width=1200px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/data_connection_langchain.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Retrieval | LangChain**ü¶úÔ∏èüîó](https://python.langchain.com/v0.1/docs/modules/data_connection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XaZ20XoeSTD-",
   "metadata": {
    "id": "XaZ20XoeSTD-"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** RAG for Conversation History\n",
    "\n",
    "In our previous explorations, we delved into the capabilities of document embedding models and used them to embed, store, and compare semantic vector representations of text. Though we could motivate how to efficiently extend this into vector store land manually, the true beauty of working with a standard API is its strong incorporation with other frameworks that can already do the heavy lifting for us!\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LRx0XUf_Sdxw",
   "metadata": {
    "id": "LRx0XUf_Sdxw"
   },
   "source": [
    "### **Step 1**: Getting A Conversation\n",
    "\n",
    "Consider a conversation crafted using Llama-13B between a chat agent and a blue bear named Beras. This dialogue, dense with details and potential diversions, provides a rich dataset for our study:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "IUfCuMkoShWI",
   "metadata": {
    "id": "IUfCuMkoShWI"
   },
   "outputs": [],
   "source": [
    "conversation = [  ## This conversation was generated partially by an AI system, and modified to exhibit desirable properties\n",
    "    \"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\",\n",
    "    \"[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch across North America\",\n",
    "    \"[Beras] Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard many great things about them.\",\n",
    "    \"[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for you!\"\n",
    "    \"[Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.\",\n",
    "    \"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research online or watching documentaries about them.\"\n",
    "    \"[Beras] I live in the arctic, so I'm not used to the warm climate there. I was just curious, ya know!\",\n",
    "    \"[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains and their significance!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tDL2tAo2Skh2",
   "metadata": {
    "id": "tDL2tAo2Skh2"
   },
   "source": [
    "Using the manual embedding strategy from the previous notebook is still very viable, but we can also rest easy and let a **vector store** do all that work for us!\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5hIp943mSqGZ",
   "metadata": {
    "id": "5hIp943mSqGZ"
   },
   "source": [
    "### **Step 2:** Constructing Our Vector Store Retriever\n",
    "\n",
    "To streamline similarity queries on our conversation, we can employ a vector store to help keep track of passages for us! **Vector Stores**, or vector storage systems, abstract away most of the low-level details of the embedding/comparison strategies and provide a simple interface to load and compare vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pnaOBgexS-kp",
   "metadata": {
    "id": "pnaOBgexS-kp"
   },
   "source": [
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1ZjwYbSZzsXK6ZP8O1-cY3BeRffV4oqzb\" width=1000px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/vector_stores.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Vector Stores | LangChain**ü¶úÔ∏èüîó](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DwZUh6kgS5Ki",
   "metadata": {
    "id": "DwZUh6kgS5Ki"
   },
   "source": [
    "<br>\n",
    "\n",
    "In addition to simplifying the process from an API perspective, vector stores also implement connectors, integrations, and optimizations under the hood. In our case, we will start with the [**FAISS vector store**](https://python.langchain.com/docs/integrations/vectorstores/faiss), which integrates a LangChain-compatable Embedding model with the [**FAISS (Facebook AI Similarity Search)**](https://github.com/facebookresearch/faiss) library to make the process fast and scalable on our local machine!\n",
    "\n",
    "**Specifically:**\n",
    "\n",
    "1. We can feed our conversation into [**a FAISS vector store**](https://python.langchain.com/docs/integrations/vectorstores/faiss) via the `from_texts` constructor. This will take our conversational data and the embedding model to create a searchable index over our discussion.\n",
    "2. This vector store can then be \"interpreted\" as a retriever, supporting the LangChain runnable API and returning documents retrieved via an input query.\n",
    "\n",
    "The following shows how you can construct a FAISS vector store and reinterpret it as a retriever using the LangChain `vectorstore` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1kE2-ejoTKKU",
   "metadata": {
    "id": "1kE2-ejoTKKU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.8 ms, sys: 4.95 ms, total: 26.7 ms\n",
      "Wall time: 529 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ^^ This cell will be timed to see how long the conversation embedding takes\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "## Streamlined from_texts FAISS vectorstore construction from text list\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "retriever = convstore.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muN66v5PW5dW",
   "metadata": {
    "id": "muN66v5PW5dW"
   },
   "source": [
    "The retriever can now be used like any other LangChain runnable to query the vector store for some relevant documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "kNZJTnlEWVYh",
   "metadata": {
    "id": "kNZJTnlEWVYh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001b[0m\n",
       "\u001b[32mrocky mountains?\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001b[0m\n",
       "\u001b[32mand their significance!'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"What is your name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "SE1eDZTEWScC",
   "metadata": {
    "id": "SE1eDZTEWScC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">across North America'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m The Rocky Mountains are a beautiful and majestic range of mountains that stretch \u001b[0m\n",
       "\u001b[32macross North America'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001b[0m\n",
       "\u001b[32monline or watching documentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate \u001b[0m\n",
       "\u001b[32mthere. I was just curious, ya know!\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mtNCEXLYTVf4",
   "metadata": {
    "id": "mtNCEXLYTVf4"
   },
   "source": [
    "As we can see, our retriever found a handful of semantically relevant documents from our query. You may notice that not all of the documents are useful or clear on their own. For example, a retrieval of *\"Beras\"* for *\"your name\"* may be problematic for the chatbot if provided out of context. Anticipating the potential problems and creating synergies between your LLM components can increase the likelihood of good RAG behavior, so keep an eye out for such pitfalls and opportunities.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZEDEzpqmTYMv",
   "metadata": {
    "id": "ZEDEzpqmTYMv"
   },
   "source": [
    "### **Step 3:** Incorporating Conversation Retrieval Into Our Chain\n",
    "\n",
    "Now that we have our loaded retriever component as a chain, we can incorporate it into our existing chat system as before. Specifically, we can start with an ***always-on RAG formulation*** where:\n",
    "- **A retriever is always retrieving context by default**.\n",
    "- **A generator is acting on the retrieved context**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64abe478-9bcb-4802-a26e-dc5a1756e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Utility Runnables/Methods\n",
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        if preface: print(preface, end=\"\")\n",
    "        pprint(x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "## Optional; Reorders longer documents to center of output text\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "uue5UY3_TcvF",
   "metadata": {
    "id": "uue5UY3_TcvF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Based on the information provided, Beras lives in the Arctic.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mBased on the information provided, Beras lives in the Arctic.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {question}\"\n",
    "    \"\\nAnswer the user conversationally. User is not aware of context.\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever(search_kwargs={'k':2}) | long_reorder | docs2str,\n",
    "        'question': (lambda x:x)\n",
    "    }\n",
    "    | context_prompt\n",
    "    # | RPrint()\n",
    "    | instruct_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint(chain.invoke(\"Where does Beras live?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FSIqTMuuTjIh",
   "metadata": {
    "id": "FSIqTMuuTjIh"
   },
   "source": [
    "Take a second to try out some more invocations and see how the new setup performs. Regardless of your model choice, the following questions should serve as interesting starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4jDJwrYpTmpd",
   "metadata": {
    "id": "4jDJwrYpTmpd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Rocky Mountains are a range of mountains that stretches across North America. If you're not familiar with them,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">you can learn more by doing some research online or watching documentaries about them!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mThe Rocky Mountains are a range of mountains that stretches across North America. If you're not familiar with them,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0myou can learn more by doing some research online or watching documentaries about them!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "-artagLfTpBy",
   "metadata": {
    "id": "-artagLfTpBy"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Rocky Mountains are indeed a beautiful and majestic range of mountains, stretching across North America. To </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">answer your question, they are not close to California. They actually span through several countries and states, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">including the U.S. states of Colorado, Idaho, Montana, Wyoming, Utah, and New Mexico. You can learn more about the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Rocky Mountains by doing some research online or watching documentaries about them. Since you mentioned living in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the Arctic, the Rockies might be quite a different climate for you!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mThe Rocky Mountains are indeed a beautiful and majestic range of mountains, stretching across North America. To \u001b[0m\n",
       "\u001b[1;38;2;118;185;0manswer your question, they are not close to California. They actually span through several countries and states, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mincluding the U.S. states of Colorado, Idaho, Montana, Wyoming, Utah, and New Mexico. You can learn more about the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mRocky Mountains by doing some research online or watching documentaries about them. Since you mentioned living in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe Arctic, the Rockies might be quite a different climate for you!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains? Are they close to California?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "GDgjdfdpTrV5",
   "metadata": {
    "id": "GDgjdfdpTrV5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Based on the information provided, I don't have enough details to give you the exact distance between Beras' </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">location in the Arctic and the Rocky Mountains. However, I can tell you that the Rocky Mountains are a significant </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">distance away from the Arctic, as they are located in North America and extend from the northernmost part of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">British Columbia, in western Canada, to New Mexico in the Southwestern United States.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mBased on the information provided, I don't have enough details to give you the exact distance between Beras' \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlocation in the Arctic and the Rocky Mountains. However, I can tell you that the Rocky Mountains are a significant \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdistance away from the Arctic, as they are located in North America and extend from the northernmost part of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mBritish Columbia, in western Canada, to New Mexico in the Southwestern United States.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"How far away is Beras from the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8wp9-8CbT0L9",
   "metadata": {
    "id": "8wp9-8CbT0L9"
   },
   "source": [
    "<br>\n",
    "\n",
    "You might notice some decent performance with this always-on retrieval node in the loop since the actual context being fed into the LLM remains relatively small. It's important to experiment with factors like embedding sizes, context limits, and model options to see what kinds of behavior you can expect and which efforts are worth taking to improve performance.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OnpOybOhUCTf",
   "metadata": {
    "id": "OnpOybOhUCTf"
   },
   "source": [
    "### **Step 4:** Automatic Conversation Storage\n",
    "\n",
    "Now that we see how our vector store memory unit should function, we can perform one last integration to allow our conversation to add new entries to our conversation: a runnable that calls the `add_texts` method for us to update the store state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "FsK6-AtRVdcZ",
   "metadata": {
    "id": "FsK6-AtRVdcZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">I'm glad to hear you're excited about visiting the Rocky Mountains, Beras! However, I must admit I'm a bit </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">confused. There aren't any ice cream stores in the Rockies, as far as I know. But rest assured, there are plenty of</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">other amazing things to see and experience there, like the beautiful mountain landscapes and wildlife. I'm sure </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">you'll have a great adventure!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mI'm glad to hear you're excited about visiting the Rocky Mountains, Beras! However, I must admit I'm a bit \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconfused. There aren't any ice cream stores in the Rockies, as far as I know. But rest assured, there are plenty of\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mother amazing things to see and experience there, like the beautiful mountain landscapes and wildlife. I'm sure \u001b[0m\n",
       "\u001b[1;38;2;118;185;0myou'll have a great adventure!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Based on what you've said earlier, it seems like you mentioned your excitement about visiting the Rocky Mountains </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and looked forward to having ice cream there. So, I'm going to take a guess and say that your favorite food might </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">be ice cream! It's a popular treat and can be enjoyed in many different flavors.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mBased on what you've said earlier, it seems like you mentioned your excitement about visiting the Rocky Mountains \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand looked forward to having ice cream there. So, I'm going to take a guess and say that your favorite food might \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbe ice cream! It's a popular treat and can be enjoyed in many different flavors.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Oh, I see! I'm glad to know that honey is your favorite food, Beras. I must admit, I might have gotten a bit </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">carried away with my guess. I thought you were looking forward to having ice cream in the Rocky Mountains, but I'm </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">happy to hear that's not the case. Honey is a great choice too - it's delicious and can be used in so many </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">different ways. Do you have a favorite type of honey, or do you enjoy all kinds?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mOh, I see! I'm glad to know that honey is your favorite food, Beras. I must admit, I might have gotten a bit \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcarried away with my guess. I thought you were looking forward to having ice cream in the Rocky Mountains, but I'm \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhappy to hear that's not the case. Honey is a great choice too - it's delicious and can be used in so many \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdifferent ways. Do you have a favorite type of honey, or do you enjoy all kinds?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Based on our conversation, I believe your favorite food is honey! You mentioned that it's actually your favorite, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">so I'm guessing ice cream isn't your top choice after all. Do you have a particular type of honey that you enjoy </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the most?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mBased on our conversation, I believe your favorite food is honey! You mentioned that it's actually your favorite, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mso I'm guessing ice cream isn't your top choice after all. Do you have a particular type of honey that you enjoy \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe most?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Reset knowledge base and define what it means to add more messages.\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([f\"User said {d.get('input')}\", f\"Agent said {d.get('output')}\"])\n",
    "    return d.get('output')\n",
    "\n",
    "########################################################################\n",
    "\n",
    "# instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    "    \"\\nAnswer the user conversationally. Make sure the conversation flows naturally.\\n\"\n",
    "    \"[Agent]\"\n",
    ")\n",
    "\n",
    "\n",
    "conv_chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'input': (lambda x:x)\n",
    "    }\n",
    "    | RunnableAssign({'output' : chat_prompt | instruct_llm | StrOutputParser()})\n",
    "    | partial(save_memory_and_get_output, vstore=convstore)\n",
    ")\n",
    "\n",
    "pprint(conv_chain.invoke(\"I'm glad you agree! I can't wait to get some ice cream there! It's such a good food!\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Can you guess what my favorite food is?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Actually, my favorite is honey! Not sure where you got that idea?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"I see! Fair enough! Do you know my favorite food now?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KRMW6G7NVSWF",
   "metadata": {
    "id": "KRMW6G7NVSWF"
   },
   "source": [
    "Unlike the more automatic full-text or rule-based approaches to injecting context into the LLM, this approach ensures some amount of consolidation which can keep the context length from getting out of hand. It's still not a full-proof strategy on its own, but it's a stark improvement for unstructured conversations (and doesn't even require a strong instruction-tuned model to perform slot-filling)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9TPkh3SaLbqh",
   "metadata": {
    "id": "9TPkh3SaLbqh"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3 [Exercise]:** RAG For Document Chunk Retrieval\n",
    "\n",
    "Given our prior exploration of document loading, the idea that data chunks can be embedded and searched through probably isn't surprising. With that said, it is definitely worth going over since applying RAG with documents is a double-edged sword; it may **seem** to work well out of the box but requires some extra care when optimizing it for truly reliable performance. It also provides an excellent opportunity to review some fundamental LCEL skills, so let's see what we can do!\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Exercise:**\n",
    "\n",
    "In the previous example, you may recall that we pulled in some relatively small papers with the help of [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv) using the following syntax:\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "docs = [\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL\n",
    "    ArxivLoader(query=\"2210.03629\").load(),  ## ReAct\n",
    "]\n",
    "```\n",
    "\n",
    "Given all that you've learned so far, choose a selection of papers that you would like to use and develop a chatbot that can talk about them!\n",
    "\n",
    "<br>\n",
    "\n",
    "Though this is a pretty big task, a walkthrough of ***most*** of the process will be provided below. By the end of the walkthrough, many of the necessary puzzle pieces will be provided, and your real task will be to integrate them together for the final `retrieval_chain`. When you're done, get ready to re-integrate the chain (or a flavor of your choice) in the last notebook as part of the evaluation exercise!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jSjfCtiQnj9e",
   "metadata": {
    "id": "jSjfCtiQnj9e"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 1**: Loading And Chunking Your Documents\n",
    "\n",
    "The following code block gives you some default papers to load in for your RAG chain. Feel free to select more papers as desired, but note that longer documents will take longer to process. A few simplifying assumptions and additional processing steps are included to help you improve your naive RAG performance:\n",
    "\n",
    "- Documents are cut off prior to the \"References\" section if one exists. This will keep our system from considering the citations and appendix sections, which tend to be long and distracting.\n",
    "\n",
    "- A chunk that lists the available documents is inserted to provide a high-level view of all available documents in a single chunk. If your pipeline does not provide metadata on each retrieval, this is a useful component and can even be listed among a list of higher-priority pieces if appropriate.\n",
    "\n",
    "- Additionally, the metadata entries are also inserted to provide general information. Ideally, there would also be some synthetic chunks that merge the metadata into interesting cross-document chunks.\n",
    "\n",
    "**NOTE:** ***For the sake of the assessment, please include at least one paper that is less than one month old!***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "S-3FBdT_lhVT",
   "metadata": {
    "id": "S-3FBdT_lhVT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Documents\n",
      "Chunking Documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Available Documents:</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Attention Is All You Need</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sources and discrete reasoning</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Mistral 7B</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - A Closer Look on Gender Stereotypes in Movie Recommender Systems and Their Implications with Privacy</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - TrustRAG: Enhancing Robustness and Trustworthiness in RAG </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAvailable Documents:\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Attention Is All You Need\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msources and discrete reasoning\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Mistral 7B\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - A Closer Look on Gender Stereotypes in Movie Recommender Systems and Their Implications with Privacy\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - TrustRAG: Enhancing Robustness and Trustworthiness in RAG \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0\n",
      " - # Chunks: 35\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-02'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Attention Is All You Need'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Kaiser, Illia Polosukhin'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">constituency parsing both with large and limited training data.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-08-02'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Attention Is All You Need'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz \u001b[0m\n",
       "\u001b[32mKaiser, Illia Polosukhin'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural \u001b[0m\n",
       "\u001b[32mnetworks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder \u001b[0m\n",
       "\u001b[32mthrough an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on \u001b[0m\n",
       "\u001b[32mattention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation\u001b[0m\n",
       "\u001b[32mtasks show these models to be\\nsuperior in quality while being more parallelizable and requiring \u001b[0m\n",
       "\u001b[32msignificantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation \u001b[0m\n",
       "\u001b[32mtask, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 \u001b[0m\n",
       "\u001b[32mEnglish-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 \u001b[0m\n",
       "\u001b[32mafter training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the \u001b[0m\n",
       "\u001b[32mliterature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish \u001b[0m\n",
       "\u001b[32mconstituency parsing both with large and limited training data.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1\n",
      " - # Chunks: 45\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2019-05-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Encoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">context in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">layer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">empirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">including\\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">absolute improvement), SQuAD v1.1 question answering\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2019-05-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional \u001b[0m\n",
       "\u001b[32mEncoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to \u001b[0m\n",
       "\u001b[32mpre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right \u001b[0m\n",
       "\u001b[32mcontext in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output \u001b[0m\n",
       "\u001b[32mlayer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language \u001b[0m\n",
       "\u001b[32minference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and \u001b[0m\n",
       "\u001b[32mempirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, \u001b[0m\n",
       "\u001b[32mincluding\\npushing the GLUE score to 80.5% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7.7% point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, MultiNLI\\naccuracy to 86.7% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m4.6% \u001b[0m\n",
       "\u001b[32mabsolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, SQuAD v1.1 question answering\\nTest F1 to 93.2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1.5 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and SQuAD \u001b[0m\n",
       "\u001b[32mv2.0 Test F1 to 83.1\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m5.1 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 2\n",
      " - # Chunks: 46\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-04-12'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">precisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">behind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">world knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">downstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation (RAG) -- </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">index\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">passages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2021-04-12'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, \u001b[0m\n",
       "\u001b[32mHeinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, \u001b[0m\n",
       "\u001b[32mand achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and\u001b[0m\n",
       "\u001b[32mprecisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags \u001b[0m\n",
       "\u001b[32mbehind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their \u001b[0m\n",
       "\u001b[32mworld knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism \u001b[0m\n",
       "\u001b[32mto\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive \u001b[0m\n",
       "\u001b[32mdownstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m -- \u001b[0m\n",
       "\u001b[32mmodels which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG \u001b[0m\n",
       "\u001b[32mmodels where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector \u001b[0m\n",
       "\u001b[32mindex\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which \u001b[0m\n",
       "\u001b[32mconditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different \u001b[0m\n",
       "\u001b[32mpassages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set\u001b[0m\n",
       "\u001b[32mthe state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific \u001b[0m\n",
       "\u001b[32mretrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more \u001b[0m\n",
       "\u001b[32mspecific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 3\n",
      " - # Chunks: 40\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2022-05-01'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge sources and discrete reasoning'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Amnon Shashua, Moshe Tenenholtz'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Huge language models (LMs) have ushered in a new era for AI, serving as a\\ngateway to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">natural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">systems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">linguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge and Language (MRKL, pronounced \"miracle\") system,\\nsome of the technical challenges in implementing it, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2022-05-01'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external \u001b[0m\n",
       "\u001b[32mknowledge sources and discrete reasoning'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit\u001b[0m\n",
       "\u001b[32mBata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, \u001b[0m\n",
       "\u001b[32mAmnon Shashua, Moshe Tenenholtz'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Huge language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have ushered in a new era for AI, serving as a\\ngateway to \u001b[0m\n",
       "\u001b[32mnatural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently \u001b[0m\n",
       "\u001b[32mlimited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a \u001b[0m\n",
       "\u001b[32msystems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to \u001b[0m\n",
       "\u001b[32mlinguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete \u001b[0m\n",
       "\u001b[32mknowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, \u001b[0m\n",
       "\u001b[32mKnowledge and Language \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMRKL, pronounced \"miracle\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m system,\\nsome of the technical challenges in implementing it, \u001b[0m\n",
       "\u001b[32mand Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 4\n",
      " - # Chunks: 21\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-10-10'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Mistral 7B'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention (GQA) for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">faster\\ninference, coupled with sliding window attention (SWA) to effectively handle\\nsequences of arbitrary length</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">under the Apache 2.0 license.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-10-10'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Mistral 7B'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, \u001b[0m\n",
       "\u001b[32mDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, \u001b[0m\n",
       "\u001b[32mMarie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior \u001b[0m\n",
       "\u001b[32mperformance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in\u001b[0m\n",
       "\u001b[32mreasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for \u001b[0m\n",
       "\u001b[32mfaster\\ninference, coupled with sliding window attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSWA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to effectively handle\\nsequences of arbitrary length\u001b[0m\n",
       "\u001b[32mwith a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, \u001b[0m\n",
       "\u001b[32mthat surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released \u001b[0m\n",
       "\u001b[32munder the Apache 2.0 license.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 5\n",
      " - # Chunks: 44\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-12-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conversations with\\nhuman preferences are publicly available </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-12-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, \u001b[0m\n",
       "\u001b[32mZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Evaluating large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m based chat assistants is challenging\\ndue to their broad \u001b[0m\n",
       "\u001b[32mcapabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore\u001b[0m\n",
       "\u001b[32musing strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and \u001b[0m\n",
       "\u001b[32mlimitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited \u001b[0m\n",
       "\u001b[32mreasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between \u001b[0m\n",
       "\u001b[32mLLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot \u001b[0m\n",
       "\u001b[32mArena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both \u001b[0m\n",
       "\u001b[32mcontrolled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement \u001b[0m\n",
       "\u001b[32mbetween humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which \u001b[0m\n",
       "\u001b[32mare otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement\u001b[0m\n",
       "\u001b[32meach other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K \u001b[0m\n",
       "\u001b[32mconversations with\\nhuman preferences are publicly available \u001b[0m\n",
       "\u001b[32mat\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 6\n",
      " - # Chunks: 84\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-08'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'A Closer Look on Gender Stereotypes in Movie Recommender Systems and Their Implications with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Privacy'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Falguni Roy, Yiduo Shen, Na Zhao, Xiaofeng Ding, Md. Omar Faruk'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"The movie recommender system typically leverages user feedback to provide\\npersonalized </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recommendations that align with user preferences and increase\\nbusiness revenue. This study investigates the impact</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of gender stereotypes on\\nsuch systems through a specific attack scenario. In this scenario, an </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attacker\\ndetermines users' gender, a private attribute, by exploiting gender stereotypes\\nabout movie preferences </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and analyzing users' feedback data, which is either\\npublicly available or observed within the system. The study </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">consists of two\\nphases. In the first phase, a user study involving 630 participants identified\\ngender stereotypes</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">associated with movie genres, which often influence viewing\\nchoices. In the second phase, four inference </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">algorithms were applied to detect\\ngender stereotypes by combining the findings from the first phase with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">users'\\nfeedback data. Results showed that these algorithms performed more effectively\\nthan relying solely on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">feedback data for gender inference. Additionally, we\\nquantified the extent of gender stereotypes to evaluate their</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">broader impact on\\ndigital computational science. The latter part of the study utilized two major\\nmovie </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recommender datasets: MovieLens 1M and Yahoo!Movie. Detailed experimental\\ninformation is available on our GitHub </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">repository:\\nhttps://github.com/fr-iit/GSMRS\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2025-01-08'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'A Closer Look on Gender Stereotypes in Movie Recommender Systems and Their Implications with \u001b[0m\n",
       "\u001b[32mPrivacy'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Falguni Roy, Yiduo Shen, Na Zhao, Xiaofeng Ding, Md. Omar Faruk'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m\"The movie recommender system typically leverages user feedback to provide\\npersonalized \u001b[0m\n",
       "\u001b[32mrecommendations that align with user preferences and increase\\nbusiness revenue. This study investigates the impact\u001b[0m\n",
       "\u001b[32mof gender stereotypes on\\nsuch systems through a specific attack scenario. In this scenario, an \u001b[0m\n",
       "\u001b[32mattacker\\ndetermines users' gender, a private attribute, by exploiting gender stereotypes\\nabout movie preferences \u001b[0m\n",
       "\u001b[32mand analyzing users' feedback data, which is either\\npublicly available or observed within the system. The study \u001b[0m\n",
       "\u001b[32mconsists of two\\nphases. In the first phase, a user study involving 630 participants identified\\ngender stereotypes\u001b[0m\n",
       "\u001b[32massociated with movie genres, which often influence viewing\\nchoices. In the second phase, four inference \u001b[0m\n",
       "\u001b[32malgorithms were applied to detect\\ngender stereotypes by combining the findings from the first phase with \u001b[0m\n",
       "\u001b[32musers'\\nfeedback data. Results showed that these algorithms performed more effectively\\nthan relying solely on \u001b[0m\n",
       "\u001b[32mfeedback data for gender inference. Additionally, we\\nquantified the extent of gender stereotypes to evaluate their\u001b[0m\n",
       "\u001b[32mbroader impact on\\ndigital computational science. The latter part of the study utilized two major\\nmovie \u001b[0m\n",
       "\u001b[32mrecommender datasets: MovieLens 1M and Yahoo!Movie. Detailed experimental\\ninformation is available on our GitHub \u001b[0m\n",
       "\u001b[32mrepository:\\nhttps://github.com/fr-iit/GSMRS\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 7\n",
      " - # Chunks: 45\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-01'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'TrustRAG: Enhancing Robustness and Trustworthiness in RAG'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Huichi Zhou, Kin-Hei Lee, Zhonghao Zhan, Yue Chen, Zhenhao Li'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Retrieval-Augmented Generation (RAG) systems enhance large language models\\n(LLMs) by integrating </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">external knowledge sources, enabling more accurate and\\ncontextually relevant responses tailored to user queries. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">However, these\\nsystems remain vulnerable to corpus poisoning attacks that can significantly\\ndegrade LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance through the injection of malicious content. To address\\nthese challenges, we propose TrustRAG, a robust</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">framework that systematically\\nfilters compromised and irrelevant content before it reaches the language\\nmodel. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Our approach implements a two-stage defense mechanism: first, it employs\\nK-means clustering to identify potential </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attack patterns in retrieved documents\\nbased on their semantic embeddings, effectively isolating suspicious </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">content.\\nSecond, it leverages cosine similarity and ROUGE metrics to detect malicious\\ndocuments while resolving </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">discrepancies between the model's internal knowledge\\nand external information through a self-assessment process. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">TrustRAG functions\\nas a plug-and-play, training-free module that integrates seamlessly with any\\nlanguage model, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">whether open or closed-source, maintaining high contextual\\nrelevance while strengthening defenses against attacks.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Through extensive\\nexperimental validation, we demonstrate that TrustRAG delivers substantial\\nimprovements in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieval accuracy, efficiency, and attack resistance compared\\nto existing approaches across multiple model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">architectures and datasets. We\\nhave made TrustRAG available as open-source software </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at\\n\\\\url{https://github.com/HuichiZhou/TrustRAG}.\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2025-01-01'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'TrustRAG: Enhancing Robustness and Trustworthiness in RAG'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Huichi Zhou, Kin-Hei Lee, Zhonghao Zhan, Yue Chen, Zhenhao Li'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m\"Retrieval-Augmented Generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m systems enhance large language models\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m by integrating \u001b[0m\n",
       "\u001b[32mexternal knowledge sources, enabling more accurate and\\ncontextually relevant responses tailored to user queries. \u001b[0m\n",
       "\u001b[32mHowever, these\\nsystems remain vulnerable to corpus poisoning attacks that can significantly\\ndegrade LLM \u001b[0m\n",
       "\u001b[32mperformance through the injection of malicious content. To address\\nthese challenges, we propose TrustRAG, a robust\u001b[0m\n",
       "\u001b[32mframework that systematically\\nfilters compromised and irrelevant content before it reaches the language\\nmodel. \u001b[0m\n",
       "\u001b[32mOur approach implements a two-stage defense mechanism: first, it employs\\nK-means clustering to identify potential \u001b[0m\n",
       "\u001b[32mattack patterns in retrieved documents\\nbased on their semantic embeddings, effectively isolating suspicious \u001b[0m\n",
       "\u001b[32mcontent.\\nSecond, it leverages cosine similarity and ROUGE metrics to detect malicious\\ndocuments while resolving \u001b[0m\n",
       "\u001b[32mdiscrepancies between the model's internal knowledge\\nand external information through a self-assessment process. \u001b[0m\n",
       "\u001b[32mTrustRAG functions\\nas a plug-and-play, training-free module that integrates seamlessly with any\\nlanguage model, \u001b[0m\n",
       "\u001b[32mwhether open or closed-source, maintaining high contextual\\nrelevance while strengthening defenses against attacks.\u001b[0m\n",
       "\u001b[32mThrough extensive\\nexperimental validation, we demonstrate that TrustRAG delivers substantial\\nimprovements in \u001b[0m\n",
       "\u001b[32mretrieval accuracy, efficiency, and attack resistance compared\\nto existing approaches across multiple model \u001b[0m\n",
       "\u001b[32marchitectures and datasets. We\\nhave made TrustRAG available as open-source software \u001b[0m\n",
       "\u001b[32mat\\n\\\\url\u001b[0m\u001b[32m{\u001b[0m\u001b[32mhttps://github.com/HuichiZhou/TrustRAG\u001b[0m\u001b[32m}\u001b[0m\u001b[32m.\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
    ")\n",
    "\n",
    "## TODO: Please pick some papers and add them to the list as you'd like\n",
    "## NOTE: To re-use for the final assessment, make sure at least one paper is < 1 month old\n",
    "print(\"Loading Documents\")\n",
    "docs = [\n",
    "    ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
    "    ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
    "    ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
    "    ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
    "    ArxivLoader(query=\"2306.05685\").load(),  ## LLM-as-a-Judge\n",
    "    ArxivLoader(query=\"2501.04420\").load(),   ## Gender Stereotypes in Movie Recommender Systems\n",
    "    ArxivLoader(query=\"2501.00879\").load()   ## TrustRAG\n",
    "    ## Some longer papers\n",
    "    # ArxivLoader(query=\"2210.03629\").load(),  ## ReAct Paper\n",
    "    # ArxivLoader(query=\"2112.10752\").load(),  ## Latent Stable Diffusion Paper\n",
    "    # ArxivLoader(query=\"2103.00020\").load(),  ## CLIP Paper\n",
    "    ## TODO: Feel free to add more\n",
    "]\n",
    "\n",
    "## Cut the paper short if references is included.\n",
    "## This is a standard string in papers.\n",
    "for doc in docs:\n",
    "    content = json.dumps(doc[0].page_content)\n",
    "    if \"References\" in content:\n",
    "        doc[0].page_content = content[:content.index(\"References\")]\n",
    "\n",
    "## Split the documents and also filter out stubs (overly short chunks)\n",
    "print(\"Chunking Documents\")\n",
    "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
    "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
    "\n",
    "## Make some custom Chunks to give big-picture details\n",
    "doc_string = \"Available Documents:\"\n",
    "doc_metadata = []\n",
    "for chunks in docs_chunks:\n",
    "    metadata = getattr(chunks[0], 'metadata', {})\n",
    "    doc_string += \"\\n - \" + metadata.get('Title')\n",
    "    doc_metadata += [str(metadata)]\n",
    "\n",
    "extra_chunks = [doc_string] + doc_metadata\n",
    "# extra_chunks contains only the 'Title' fields from the document metadata, which looks like the following::\n",
    "# Available Documents:\n",
    "#  - Attention Is All You Need\n",
    "#  - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "#  - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
    "#  - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge \n",
    "# sources and discrete reasoning\n",
    "#  - Mistral 7B\n",
    "#  - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n",
    "#  - A Closer Look on Gender Stereotypes in Movie Recommender Systems and Their Implications with Privacy\n",
    "#  - TrustRAG: Enhancing Robustness and Trustworthiness in RAG \n",
    "\n",
    "## Printing out some summary information for reference\n",
    "pprint(doc_string, '\\n')\n",
    "for i, chunks in enumerate(docs_chunks):      # doc_chunks contains the contents of the documents, including the 'Title' and other metadata\n",
    "    print(f\"Document {i}\")\n",
    "    print(f\" - # Chunks: {len(chunks)}\")\n",
    "    print(f\" - Metadata: \")\n",
    "    pprint(chunks[0].metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4pWU_OOnnrsT",
   "metadata": {
    "id": "4pWU_OOnnrsT"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 2**: Construct Your Document Vector Stores\n",
    "\n",
    "Now that we have all of the components, we can go ahead and create indices surrounding them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "lwwmr3aptwCg",
   "metadata": {
    "id": "lwwmr3aptwCg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Vector Stores\n",
      "CPU times: user 914 ms, sys: 38.5 ms, total: 952 ms\n",
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Constructing Vector Stores\")\n",
    "vecstores = [FAISS.from_texts(extra_chunks, embedder)]     # vector store for the metadata (Title)\n",
    "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]      # vector store for the contents, including all metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j39JwCKubto0",
   "metadata": {
    "id": "j39JwCKubto0"
   },
   "source": [
    "<br>\n",
    "\n",
    "From there, we can combine our indices into a single one using the following utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "Q7us66iPVc70",
   "metadata": {
    "id": "Q7us66iPVc70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 369 chunks\n"
     ]
    }
   ],
   "source": [
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )\n",
    "\n",
    "def aggregate_vstores(vectorstores):\n",
    "    ## Initialize an empty FAISS Index and merge others into it\n",
    "    ## We'll use default_faiss for simplicity, though it's tied to your embedder by reference\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore\n",
    "\n",
    "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
    "docstore = aggregate_vstores(vecstores)\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VU_VEx2mqJUK",
   "metadata": {
    "id": "VU_VEx2mqJUK"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 3: [Exercise]** Implement Your RAG Chain\n",
    "\n",
    "Finally, all the puzzle pieces are in place to implement the RAG pipeline! As a review, we now have:\n",
    "\n",
    "- A way to construct a from-scratch vector store for conversational memory (and a way to initialize an empty one with `default_FAISS()`)\n",
    "\n",
    "- A vector store pre-loaded with useful document information from our `ArxivLoader` utility (stored in `docstore`).\n",
    "\n",
    "With the help of a couple more utilities, you're finally ready to integrate your chain! A few additional convenience utilities are provided (`doc2str` and the now-common `RPrint`) but are optional to use. Additionally, some starter prompts and structures are also defined.\n",
    "\n",
    "> **Given all of this:** Please implement the `retrieval_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "-RXSrb1GcNff",
   "metadata": {
    "id": "-RXSrb1GcNff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"You are a document chatbot. Help the user as they ask questions about documents. User messaged</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">just asked: Tell me about the use of large language models as judge\\n\\n From this, we have retrieved the following </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">potentially-useful info:  Conversation History Retrieval:\\n\\n\\n Document Retrieval:\\n['Q', 'o', 'e', 'f', 'o', ' ',</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'u', 'g', 'n', ' ', 'L', '-', 's', 'a', 'J', 'd', 'e', 'w', 't', ' ', 'T', 'B', 'n', 'h', 'a', 'd', 'C', 'a', 'b', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'t', 'A', 'e', 'a', ' ', ' ', 'e', 't', ' ', 'e', 'd', 's', 'u', 's', 't', 'e', 'u', 'e', 'a', 'd', 'l', 'm', 't', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'t', 'o', 's', 'o', ' ', 'L', '-', 's', 'a', 'j', 'd', 'e', '\\\\\\\\', '3', '1', 'n', 'y', 'e', ' ', 'f', 'L', 'M', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'a', '-', '-', 'u', 'g', '\\\\\\\\', 'W', ' ', 'r', 'p', 's', ' ', ' ', 'L', '-', 's', 'a', 'j', 'd', 'e', 'v', 'r', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'a', 'i', 'n', '.', 'T', 'e', ' ', 'a', ' ', 'e', 'i', 'p', 'e', 'e', 't', 'd', 'i', 'd', 'p', 'n', 'e', 't', 'y', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'o', ' ', 'n', 'c', 'm', 'i', 'a', 'i', 'n', '\\\\\\\\', '\\\\\\\\', '2', '2', ' ', 'a', 'r', 'i', 'e', 'c', 'm', 'a', 'i',</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'o', '.', 'A', ' ', 'L', ' ', 'u', 'g', ' ', 's', 'p', 'e', 'e', 't', 'd', 'w', 't', ' ', ' ', 'u', 's', 'i', 'n', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'a', 'd', 't', 'o', 'a', 's', 'e', 's', ' ', 'n', ' ', 'a', 'k', 'd', 'n', 'o', 'd', 't', 'r', 'i', 'e', 'w', 'i', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'h', 'o', 'e', 'i', ' ', 'e', 't', 'r', 'o', ' ', 'e', 'l', 'r', ' ', ' ', 'i', '.', 'T', 'e', 'p', 'o', 'p', ' ', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'s', 'd', 'i', ' ', 'i', 'e', ' ', 'n', 'F', 'g', 'r', ' ', ' ', 'A', 'p', 'n', 'i', ')', '\\\\\\\\', '\\\\\\\\', '2', '2',</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">' ', 'i', 'g', 'e', 'a', 's', 'e', ' ', 'r', 'd', 'n', '.', 'A', 't', 'r', 'a', 'i', 'e', 'y', ' ', 'n', 'L', 'M', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'j', 'd', 'e', 'i', ' ', 's', 'e', ' ', 'o', 'd', 'r', 'c', 'l', ' ', 's', 'i', 'n', 'a', 's', 'o', 'e', 't', ' ', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'\\\\\\\\', 's', 'n', 'l', ' ', 'n', 'w', 'r', ' ', 'h', ' ', 'r', 'm', 't', 'u', 'e', ' ', 'o', ' ', 'h', 's', 's', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'e', 'a', 'i', ' ', 's', 'i', ' ', 'i', 'u', 'e', '6', '(', 'p', 'e', 'd', 'x', '.', 'n', 'u', '0', '2', 'R', 'f', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'r', 'n', 'e', 'g', 'i', 'e', ' ', 'r', 'd', 'n', '.', 'I', ' ', 'e', 't', 'i', ' ', 'a', 'e', ',', 'i', ' ', 'a', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">' ', 'e', 'b', 'n', 'f', 'c', 'a', ' ', 'o', 'p', 'o', 'i', 'e', 'a', 'r', 'f', 'r', 'n', 'e', 's', 'l', 't', 'o', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'\\\\\\\\', 'i', ' ', 'p', 'l', 'c', 'b', 'e', ' ', 'n', 'e', 'a', 'p', 'e', 'p', 'o', 'p', ' ', 'e', 'u', 'e', 'f', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'r', 'g', 'a', 'i', 'g', 'm', 't', ' ', 'r', 'b', 'e', 's', 'i', ' ', 'n', 'F', 'g', 'r', ' ', ' ', 'A', 'p', 'n', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'i', ')', '\\\\\\\\', 'T', 'e', 'e', 'm', 't', 'o', 's', 'h', 'v', ' ', 'i', 'f', 'r', 'n', ' ', 'r', 's', 'a', 'd', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'c', 'n', '\\\\n', 'Q', 'o', 'e', 'f', 'o', ' ', 'u', 'g', 'n', ' ', 'L', '-', 's', 'a', 'J', 'd', 'e', 'w', 't', ' </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">', 'T', 'B', 'n', 'h', 'a', 'd', 'C', 'a', 'b', 't', 'A', 'e', 'a', ' ', '\\\\\\\\', 'T', 'e', 'e', 'm', 't', 'o', 's',</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'h', 'v', ' ', 'i', 'f', 'r', 'n', ' ', 'r', 's', 'a', 'd', 'c', 'n', '.', 'F', 'r', 'e', 'a', 'p', 'e', ' ', 'h', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">' ', 'a', 'r', 'i', 'e', 'c', 'm', 'a', 'i', 'o', ' ', 'a', ' ', 'a', 'k', 'n', 'c', 'l', 'b', 'l', 't', ' ', 'h', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'n', 't', 'e', 'n', 'm', 'e', ' ', 'f', 'p', 'a', 'e', 's', 'i', 'c', 'e', 's', 's', ' ', 'i', 'e', ' ', 'h', 't', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'t', 'e', 'n', 'm', 'e', ' ', 'f', 'p', 's', 'i', 'l', ' ', 'a', 'r', ' ', 'r', 'w', '\\\\\\\\', 'q', 'a', 'r', 't', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'c', 'l', 'y', ' ', 'i', 'g', 'e', 'a', 's', 'e', ' ', 'r', 'd', 'n', ' ', 'a', ' ', 'e', 'u', 'a', 'l', ' ', 'o', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'d', 's', 'e', 'n', 's', 'b', 'l', ' ', 'i', 'f', 'r', 'n', 'e', ' ', 'e', 'w', 'e', ' ', 'p', 'c', 'f', 'c', 'n', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'a', 'r', ',', 'a', 'd', 'i', 's', 'r', 's', 'l', 's', 'm', 'y', 'b', 'c', 'm', ' ', 'n', 't', 'b', 'e', ' ', 's', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'a', 's', 'l', 't', ' ', 'c', 'r', 's', 'a', 'e', 'l', 'k', 'l', ' ', 'o', 'f', 'u', 't', 'a', 'e', 'm', 'r', ' ', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'h', 'n', 'r', 'l', 't', 'v', '\\\\\\\\', 'p', 'i', 'w', 's', ' ', 'e', 'u', 't', ' ', 'f', 't', 'e', 'j', 'd', 'e', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'m', 'd', 'l', 'c', 'a', 'g', 's', '\\\\\\\\', '3', '2', 'n', 'd', 'a', 't', 'g', 's', 'o', ' ', 'L', '-', 's', 'a', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'J', 'd', 'e', 'n', 'L', '-', 's', 'a', 'j', 'd', 'e', 'o', 'f', 'r', ' ', 'w', ' ', 'e', ' ', 'e', 'e', 'i', 's', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">' ', 'c', 'l', 'b', 'l', 't', ' ', 'n', ' ', 'x', 'l', 'i', 'a', 'i', 'i', 'y', ' ', 't', 'r', 'd', 'c', 's', 't', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'e', 'n', 'e', ' ', 'o', ' ', 'u', 'a', '\\\\\\\\', 'i', 'v', 'l', 'e', 'e', 't', ' ', 'n', 'b', 'i', 'g', 's', 'a', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'a', 'l', ' ', 'e', 'c', 'm', 'r', 's', 'a', 'd', 'f', 's', ' ', 't', 'r', 't', 'o', 's', ' ', 'd', 'i', 'i', 'n', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'l', 'y', ' ', 'L', ' ', 'u', 'g', 's', 'p', 'o', 'i', 'e', 'n', 'o', ' ', 'n', 'y', 's', 'o', 'e', ' ', 'u', ' ', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'l', 'o', 'e', 'p', 'a', 'a', 'i', 'n', ',', 'm', 'k', 'n', ' ', 'h', 'i', ' ', 'u', 'p', 't', ' ', 'n', 'e', 'p', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'e', 'a', 'l', ',', 'a', ' ', 'h', 'w', ' ', 'n', 'F', 'g', 'r', ' ', '.', 'n', '.', '\\\\\\\\', 'L', 'm', 't', 't', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'o', 's', 'o', ' ', 'L', '-', 's', 'a', 'J', 'd', 'e', 'n', 'e', 'i', 'e', 't', 'f', ' ', 'e', 't', 'i', ' ', 'i', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'s', 's', 'a', 'd', 'l', 'm', 't', 't', 'o', 's', 'o', ' ', 'L', ' ', 'u', 'g', 's', '[', 'u', 't', ' ', 'r', 'm', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'J', 'd', 'i', 'g', 'L', 'M', 'a', '-', '-', 'u', 'g', ' ', 'i', 'h', 'M', '-', 'e', 'c', ' ', 'n', ' ', 'h', 't', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'o', ' ', 'r', 'n', ']', '.', 'W', ' ', 'h', 'n', 'v', 'r', 'f', ' ', 'h', ' ', 'g', 'e', 'm', 'n', ' ', 'e', 'w', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'e', ' ', 'L', ' ', 'u', 'g', 's', 'a', 'd', 'h', 'm', 'n', 'p', 'e', 'e', 'e', 'c', 's', 'n', 'y', 'i', 't', 'o', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'u', 'i', 'g', 't', 'o', 'b', 'n', 'h', 'a', 'k', ':', 'M', '-', 'e', 'c', ',', 'a', 'm', 'l', 'i', 't', 'r', ' ', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'u', 's', 'i', 'n', 's', 't', ' ', 'n', ' ', 'h', 't', 'o', '\\\\\\\\', 'A', 'e', 'a', ' ', ' ', 'r', 'w', 's', 'u', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'c', 'd', 'b', 't', 'l', ' ', 'l', 't', 'o', 'm', ' ', 'u', ' ', 'e', 'u', 't', ' ', 'e', 'e', 'l', 't', 'a', ' ', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'t', 'o', 'g', 'L', 'M', 'j', 'd', 'e', '\\\\\\\\', 'l', 'k', ' ', 'P', '-', ' ', 'a', ' ', 'a', 'c', ' ', 'o', 'h', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'c', 'n', 'r', 'l', 'e', ' ', 'n', ' ', 'r', 'w', 's', 'u', 'c', 'd', 'h', 'm', 'n', 'p', 'e', 'e', 'e', 'c', 's', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'w', 'l', ',', 'n', 'c', 'i', 'v', 'n', ' ', 'v', 'r', '8', '%', 'a', 'r', 'e', 'e', 't', ' ', 'h', ' ', 'a', 'e', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'l', 'v', 'l', 'o', ' ', 'g', 'e', 'm', 'n', ' ', 'e', 'w', 'e', ' ', 'u', 'a', 's', '\\\\\\\\', 'H', 'n', 'e', ' ', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'L', '-', 's', 'a', 'j', 'd', 'e', 'i', ' ', ' ', 'c', 'l', 'b', 'e', 'a', 'd', 'e', 'p', 'a', 'n', 'b', 'e', 'w', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'y', 't', ' ', 'p', 'r', 'x', 'm', 't', ' ', 'u', 'a', '\\\\\\\\', 'p', 'e', 'e', 'e', 'c', 's', ' ', 'h', 'c', ' ', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'r', ' ', 't', 'e', 'w', 's', ' ', 'e', 'y', 'e', 'p', 'n', 'i', 'e', 't', ' ', 'b', 'a', 'n', ' ', 'd', 'i', 'i', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'n', 'l', 'y', ' ', 'e', 's', 'o', '\\\\\\\\', 'o', 'r', 'b', 'n', 'h', 'a', 'k', 'a', 'd', 't', 'a', 'i', 'i', 'n', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'l', 'b', 'n', 'h', 'a', 'k', ' ', 'o', 'p', 'e', 'e', 't', 'e', 'c', ' ', 't', 'e', ' ', 'y', 'e', 'a', 'u', 't', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'n', '\\\\\\\\', 's', 'v', 'r', 'l', 'v', 'r', 'a', 't', ' ', 'f', 'L', 'a', 'A', 'a', 'd', 'V', 'c', 'n', '.', 'T', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'e', 'M', '-', 'e', 'c', ' ', 'u', 's', 'i', 'n', ',', '3', ' ', 'x', 'e', 't', 'v', 't', 's', '\\\\\\\\', 'a', 'd', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'3', 'K', 'c', 'n', 'e', 's', 't', 'o', 's', 'w', 't', ' ', 'u', 'a', ' ', 'r', 'f', 'r', 'n', 'e', ' ', 'r', ' ', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'u', 'l', 'c', 'y', 'a', 'a', 'l', 'b', 'e', 'a', ' ', 't', 'p', ':', 'n', '/', 'i', 'h', 'b', 'c', 'm', 'l', '-', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'y', '/', 'a', 't', 'h', 't', 't', 'e', '/', 'a', 'n', 'f', 's', 'c', 'a', '/', 'l', '_', 'u', 'g', '\\\\n', 'e', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'d', 'j', 'm', 'l', 't', 'h', 't', 'a', '/', 'i', 'm', 'e', 'r', '/', 'a', 'C', 's', 'F', 's', 's', 'm', '/', 'o', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'.', 'u', 't', 'g', '/', '\\\\\\\\', 's', 't', 'h', 't', ' ', 'l', 'a', 'i', 'v', ' ', 'l', 'i', 'b', 'p', 'e', 'a', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'s', 'c', 'e', 'e', 'e', 'p', 'n', 'm', 'h', 'h', 'i', ' ', 'n', 'i', 'a', 'r', 'v', 'o', ' ', '0', ' ', 'n', 'n', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">',', 'e', 'o', ' ', 'r', 'p', 'e', 'K', ' ', 's', 'o', 't', 'e', 'q', 'h', 'n', 'b', 'T', ' ', 'h', ' ', 'a', 'u', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'i', ' ', 'n', ' ', 'M', 'L', ' ', 'o', 's', 'n', 'i', 'a', ' ', 'a', 'e', 'e', 'n', 'g', 'i', 'a', 'l', 'v', ' ', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'b', 'r', 'h', 'o', 'h', 'a', ' ', 'n', 'm', 'l', 'm', 'c', 's', 'r', 'm', 'c', 'e', ' ', 'a', 'o', 't', 'd', 'r', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">' ', 'n', ' ', 'r', 'm', 'c', 'e', ' ', 'u', 'n', 'w', 'h', ' ', 'w', ',', 'l', 'a', 'o', 't', 'd', 'A', '.', 'i', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'t', 'o', 'o', ' ', 'v', 's', 'e', 'x', ' ', 'r', 'v', 'e', 'i', 'r', 'h', 'o', 'e', 'a', 'h', 'i', 'w', ',', 'e', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'n', 'r', 'f', 'r', 'n', 'n', 'm', 'h', 'e', 'a', 'i', 'o', 'p', 'a', 'o', ' ', 'a', ' ', 'l', 'a', 'i', 'l', 'x', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">' ', 'n', ' ', 'l', 'a', 'a', 's', 'a', 's', ' ', 'g', 'u', '-', '-', 'a', 'M', 'L', ',', 'c', 'e', 'n', '.', 'n', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'m', 'h', 'n', 'e', 't', 'b', 't', 'e', 'e', 'r', 'a', 'f', ' ', 'e', 'e', ' ', 'm', 's', 'e', 't', ',', 'n', 'm', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'e', 'g', ' ', '0', ' ', 'e', 'o', 'g', 'i', 'e', 'h', 'a', '\\\\\\\\', 'l', 'e', ' ', 'e', 'n', 'r', 'f', 'r', ' ', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'a', 'u', ' ', 'e', 'r', 'o', 'd', 'o', 'c', 'd', 'a', 'd', 'l', 'o', 't', 'o', ' ', 't', 'b', 'h', 't', 'm', 'n', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'c', '4', 'T', 'G', 'e', 'i', 'n', 's', 'g', 'u', ' ', 'L', ' ', 'n', 'r', 's', 't', 'h', ' ', 'a', 'v', 'r', 's', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'l', 's', 'r', 'r', 'O', '.', 'r', 'f', 'a', 'p', 'e', 't', 'a', ' ', 'e', 'r', 'o', 'd', 'o', 'c', 'a', ',', 'n', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'r', 'n', 't', 'b', 'a', 'C', 'd', 'a', ';', 'e', ' ', 'o', 't', 'e', 'q', 'n', 'u', '-', 't', 'u', ' ', ' ', 'h', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'n', 'b', 'T', ' ', 's', 'r', 'm', 'c', 'e', ' ', 'w', ' ', 'n', 'c', 'd', 'r', 'n', ' ', 'b', '\\\\\\\\', 'e', 'n', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'r', 'f', 'r', ' ', 'a', 'u', ' ', 'n', ' ', 'e', 'd', 'j', 'M', 'L', 'n', 'e', 't', 'b', 't', 'e', 'e', 'r', 'a', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'e', 't', 'y', 'i', 'e', ' ', 'e', 't', 'e', ' ', ' ', 'a', 'e', 'A', 't', 'b', 'a', 'C', 'd', 'a', 'h', 'n', 'B', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'T', ' ', 't', 'w', 'e', 'd', 'J', 'a', 's', '-', 'L', ' ', 'n', 'g', 'u', ' ', 'o', 'f', 'e', 'o', 'Q', '\\\\n', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'e', 'd', 'j', 'M', 'L', 'f', ' ', 'n', 'i', 'a', 'i', 'i', ' ', 'n', ' ', 'e', 'a', 'b', 'n', 'a', 'r', 'c', 'y', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'i', 'n', 'd', ' ', 'W', '\\\\\\\\', 'g', 'u', '-', '-', 'a', 'M', 'L', 'f', ' ', 'n', 'i', 'a', 'i', 'i', 'n', '3', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'3', '\\\\\\\\', '1', 'e', 'u', 'i', ' ', 'i', 'n', 'o', 's', 's', ' ', 'e', 'b', 't', 'r', 'r', 't', 'i', 's', 'u', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'t', 'o', 'r', 'e', 't', 'g', 'i', 'a', ' ', 's', 'o', 't', 'n', 'l', 'x', ' ', 's', 'a', 't', 'b', 's', 'r', 'c', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">' ', 'l', 'o', 't', 'n', '\\\\\\\\', 'd', 'v', 'r', ' ', 'e', 'd', 'j', 'M', 'L', ',', 'l', 'a', 'o', 't', 'd', 'A', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'.', 'n', 'i', 'a', 'e', 'i', 't', 'a', ' ', 'n', ' ', 'k', 'a', 'h', 'n', 'b', 'e', 'b', 'l', 'c', ' ', 'n', 'l', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'a', 'e', ',', 'n', 'm', 'v', 'o', 'n', 'n', 'n', 'm', 'h', 'r', 'f', 'd', 'e', ' ', 'h', ' ', 'e', 'u', 'e', ' ', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'I', '.', 't', 'l', 'b', 'n', 'a', 'p', 'e', 'd', 'a', 'y', 'i', 'i', 'a', 'a', 's', ':', 't', 'f', 'n', 'b', 'y', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'k', 'o', 't', 's', 'e', 'f', ' ', 'g', 'u', '-', '-', 'a', 'M', 'L', '\\\\\\\\', 'g', 'u', '-', '-', 'a', 'M', 'L', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'f', ' ', 'e', 'a', 'n', 'v', 'A', '\\\\\\\\', '.', 'n', '.', 'e', 'n', 'h', ' ', 'e', 'o', ' ', 'g', 'u', ' ', 'h', ' </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">', 'i', 's', 'l', 's', 'r', 'e', 'i', 'r', 'a', 'n', 'e', 'i', 'a', 'e', ' ', 'a', 't', 'e', 'o', ' ', 't', 'u', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'c', 'l', ' ', 't', 'y', 'e', 'i', ' ', 'r', ' ', 'e', 'o', 's', 'e', 'u', 'o', 'b', ' ', 'a', ',', 'l', 'a', 's', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'u', 'e', 'o', 'e', ' ', 'a', ' ', 't', 'u', 'e', ' ', 't', ' ', 'n', ' ', 's', 'i', 'p', '\\\\\\\\', 'i', 'i', 'e', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'s', 'n', 'e', 't', 'b', 's', 'c', 'e', 'e', 'f', 'd', 'e', 't', 'u', ' ', 'r', 'c', 'i', ' ', 't', 'e', 'b', 'n', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">' ', 'b', 'y', 'm', 'g', 'i', 'a', 'g', 'r', 'w', 'n', ' ', 'l', 'n', 's', ';', 'l', 'a', 'i', 'a', 'd', 'u', 'n', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'s', 'o', 'g', 's', 'i', 'p', 'e', 'b', 's', 'o', ' ', 'o', 'r', 'b', 'u', ' ', 'h', ' ', 'a', 't', 'n', 'v', 'g', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">',', 'e', 'a', 'r', 'n', ' ', 'r', 'y', 'l', ' ', 'o', 'r', 'b', 'u', ' ', 'h', ' ', 'e', 'w', 'y', 'i', 'i', 'a', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'a', 's', '\\\\\\\\', 'c', 'l', 'y', 'm', 'n', 's', 'r', 'p', 'o', ' ', 's', 'w', 'i', 'p', 'e', 't', ',', 'l', 'm', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'x', ' ', 'o', ' ', 's', 'o', ' ', 'n', ' ', 'o', 'p', 't', 'e', 'e', 'f', 'd', 'e', 'a', ' ', 'd', 'h', 'e', ' ', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'s', 'h', 'n', '.', ']', 'n', 'r', ' ', 'o', 't', 'h', ' ', 'n', ' ', 'c', 'e', '-', 'M', 'h', 'i', ' ', 'g', 'u', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'-', '-', 'a', 'M', 'L', 'g', 'i', 'd', 'J', 'm', 'r', ' ', 't', 'u', '[', 's', 'o', ' ', 'n', ' ', 'o', 'p', 't', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'e', 'e', 'f', 'd', 'e', 'a', ' ', 'd', 'h', 'e', ' ', 's', 'h', 'n', '.', 'x', 'd', 'e', 'p', '(', '8', 'e', 'u', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'i', ' ', 'i', 's', ' ', 'm', 'l', 'o', 'p', 'h', 'a', ' ', 'n', 'd', 'r', ' ', 'o', ' ', 's', ' ', 'w', 't', 'm', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'r', ' ', 'l', 'm', 'x', ' ', 'A', '.', 'l', 'a', 'i', 'p', 'a', 'f', 'n', 'n', 'i', 'u', 'o', ' ', 'c', 'e', 'e', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'e', ' ', ' ', 'd', 'v', 'r', ' ', 't', 'l', 'i', 'i', 'e', 'e', ' ', 'b', 'y', 'm', 't', ' ', 's', 's', 'c', 'n', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'a', 'r', 'c', 'n', ' ', 'g', 'i', 'a', 'g', 'd', 'd', 'u', '-', 'c', 'e', 'e', 'e', ' ', '2', '2', '\\\\\\\\', '\\\\\\\\',</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">')', 'i', 'n', 'p', 'A', ' ', ' ', 'r', 'g', 'F', 'n', ' ', 'i', 'o', 'r', 'n', 'c', ' ', 'i', 't', 'r', 'f', 'd', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'s', ' ', 'p', 'o', 'p', 'e', 'T', '.', 'e', 's', 'a', 'e', 'g', 'i', 'n', 'a', 'o', ' ', 'r', 'c', ' ', ' ', 'g', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'s', 'a', 'y', 't', 'e', 'i', ' ', 't', 'd', 'k', 'a', 's', ' ', 'g', 'u', ' ', 'L', ' ', 'a', ',', 'l', 'v', 't', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'n', 'e', 'l', ' ', 'g', 'i', 'a', 'g', 'r', 'w', 'n', ' ', 'l', 'n', 'S', '2', '0', 'u', 'n', '.', 'x', 'd', 'e', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'p', '(', '5', 'e', 'u', 'i', ' ', 'i', 'n', 'v', 'g', 's', ' ', 'e', 'u', 't', 'm', 'r', ' ', 'h', ' ', 'e', 't', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'a', 'e', 'a', 'c', 'd', 'r', ' ', 'e', 't', 'b', 's', ' ', 'n', ' ', 'c', 'h', ' ', 'n', 'm', 'e', 'e', ' ', 't', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'\\\\\\\\', 'e', 's', 't', 'd', 'a', ',', 'r', 'w', 'n', ' ', 'w', ' ', 'n', ' ', 'o', 't', 'e', 'q', 'a', 'h', 'i', ' </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">', 'e', 'n', 's', 'r', ' ', 'i', 'e', 'd', 'j', 'M', 'L', 'n', ' ', 'n', 's', 'r', 'p', 'o', ' ', 's', 'w', 'i', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'P', '2', '0', 'u', 'n', ':', 'o', 't', 'n', 'b', 'o', ' ', 'i', 'r', ' ', 'l', 'n', 'd', 'e', 'e', 'n', ' ', 'e', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'n', 'm', 'l', 'm', ' ', 'b', 'n', 'c', 'y', 'h', ' ', 's', 'o', 't', 'i', 'a', ' ', 'g', 'u', '-', '-', 'a', 'M', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'L', '3', 'e', 'o', 'o', 'p', 'e', 'n', 'e', 'd', 'J', 'a', 's', '-', 'L', ' ', 'o', 's', 'p', 'T', '\\\\\\\\', '.', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'n', '.', 'g', 'u', '-', '-', 'a', 'M', 'L', 'f', ' ', 'n', 'i', 'a', 'i', 'i', ' ', 'n', ' ', 's', ' ', 'h', ' ', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'s', 'c', 'i', ' ', 'w', ',', 'x', 'N', '.', ']', 'n', 'r', ' ', 'o', 't', 'h', ' ', 'n', ' ', 'c', 'e', '-', 'M', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'h', 'i', ' ', 'g', 'u', '-', '-', 'a', 'M', 'L', 'g', 'i', 'd', 'J', 'm', 'r', ' ', 't', 'u', '[']\\n\\n (Answer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">only from retrieval. Only cite sources that are used. Make your response conversational.)\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about the use of large language models as judge'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mSystemMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"You\u001b[0m\u001b[32m are a document chatbot. Help the user as they ask questions about documents. User messaged\u001b[0m\n",
       "\u001b[32mjust asked: Tell me about the use of large language models as judge\\n\\n From this, we have retrieved the following \u001b[0m\n",
       "\u001b[32mpotentially-useful info:  Conversation History Retrieval:\\n\\n\\n Document Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m'Q', 'o', 'e', 'f', 'o', ' ',\u001b[0m\n",
       "\u001b[32m'u', 'g', 'n', ' ', 'L', '-', 's', 'a', 'J', 'd', 'e', 'w', 't', ' ', 'T', 'B', 'n', 'h', 'a', 'd', 'C', 'a', 'b', \u001b[0m\n",
       "\u001b[32m't', 'A', 'e', 'a', ' ', ' ', 'e', 't', ' ', 'e', 'd', 's', 'u', 's', 't', 'e', 'u', 'e', 'a', 'd', 'l', 'm', 't', \u001b[0m\n",
       "\u001b[32m't', 'o', 's', 'o', ' ', 'L', '-', 's', 'a', 'j', 'd', 'e', '\\\\\\\\', '3', '1', 'n', 'y', 'e', ' ', 'f', 'L', 'M', \u001b[0m\n",
       "\u001b[32m'a', '-', '-', 'u', 'g', '\\\\\\\\', 'W', ' ', 'r', 'p', 's', ' ', ' ', 'L', '-', 's', 'a', 'j', 'd', 'e', 'v', 'r', \u001b[0m\n",
       "\u001b[32m'a', 'i', 'n', '.', 'T', 'e', ' ', 'a', ' ', 'e', 'i', 'p', 'e', 'e', 't', 'd', 'i', 'd', 'p', 'n', 'e', 't', 'y', \u001b[0m\n",
       "\u001b[32m'o', ' ', 'n', 'c', 'm', 'i', 'a', 'i', 'n', '\\\\\\\\', '\\\\\\\\', '2', '2', ' ', 'a', 'r', 'i', 'e', 'c', 'm', 'a', 'i',\u001b[0m\n",
       "\u001b[32m'o', '.', 'A', ' ', 'L', ' ', 'u', 'g', ' ', 's', 'p', 'e', 'e', 't', 'd', 'w', 't', ' ', ' ', 'u', 's', 'i', 'n', \u001b[0m\n",
       "\u001b[32m'a', 'd', 't', 'o', 'a', 's', 'e', 's', ' ', 'n', ' ', 'a', 'k', 'd', 'n', 'o', 'd', 't', 'r', 'i', 'e', 'w', 'i', \u001b[0m\n",
       "\u001b[32m'h', 'o', 'e', 'i', ' ', 'e', 't', 'r', 'o', ' ', 'e', 'l', 'r', ' ', ' ', 'i', '.', 'T', 'e', 'p', 'o', 'p', ' ', \u001b[0m\n",
       "\u001b[32m's', 'd', 'i', ' ', 'i', 'e', ' ', 'n', 'F', 'g', 'r', ' ', ' ', 'A', 'p', 'n', 'i', '\u001b[0m\u001b[32m)\u001b[0m\u001b[32m', '\\\\\\\\', '\\\\\\\\', '2', '2',\u001b[0m\n",
       "\u001b[32m' ', 'i', 'g', 'e', 'a', 's', 'e', ' ', 'r', 'd', 'n', '.', 'A', 't', 'r', 'a', 'i', 'e', 'y', ' ', 'n', 'L', 'M', \u001b[0m\n",
       "\u001b[32m'j', 'd', 'e', 'i', ' ', 's', 'e', ' ', 'o', 'd', 'r', 'c', 'l', ' ', 's', 'i', 'n', 'a', 's', 'o', 'e', 't', ' ', \u001b[0m\n",
       "\u001b[32m'\\\\\\\\', 's', 'n', 'l', ' ', 'n', 'w', 'r', ' ', 'h', ' ', 'r', 'm', 't', 'u', 'e', ' ', 'o', ' ', 'h', 's', 's', \u001b[0m\n",
       "\u001b[32m'e', 'a', 'i', ' ', 's', 'i', ' ', 'i', 'u', 'e', '6', '\u001b[0m\u001b[32m(\u001b[0m\u001b[32m', 'p', 'e', 'd', 'x', '.', 'n', 'u', '0', '2', 'R', 'f', \u001b[0m\n",
       "\u001b[32m'r', 'n', 'e', 'g', 'i', 'e', ' ', 'r', 'd', 'n', '.', 'I', ' ', 'e', 't', 'i', ' ', 'a', 'e', ',', 'i', ' ', 'a', \u001b[0m\n",
       "\u001b[32m' ', 'e', 'b', 'n', 'f', 'c', 'a', ' ', 'o', 'p', 'o', 'i', 'e', 'a', 'r', 'f', 'r', 'n', 'e', 's', 'l', 't', 'o', \u001b[0m\n",
       "\u001b[32m'\\\\\\\\', 'i', ' ', 'p', 'l', 'c', 'b', 'e', ' ', 'n', 'e', 'a', 'p', 'e', 'p', 'o', 'p', ' ', 'e', 'u', 'e', 'f', \u001b[0m\n",
       "\u001b[32m'r', 'g', 'a', 'i', 'g', 'm', 't', ' ', 'r', 'b', 'e', 's', 'i', ' ', 'n', 'F', 'g', 'r', ' ', ' ', 'A', 'p', 'n', \u001b[0m\n",
       "\u001b[32m'i', '\u001b[0m\u001b[32m)\u001b[0m\u001b[32m', '\\\\\\\\', 'T', 'e', 'e', 'm', 't', 'o', 's', 'h', 'v', ' ', 'i', 'f', 'r', 'n', ' ', 'r', 's', 'a', 'd', \u001b[0m\n",
       "\u001b[32m'c', 'n', '\\\\n', 'Q', 'o', 'e', 'f', 'o', ' ', 'u', 'g', 'n', ' ', 'L', '-', 's', 'a', 'J', 'd', 'e', 'w', 't', ' \u001b[0m\n",
       "\u001b[32m', 'T', 'B', 'n', 'h', 'a', 'd', 'C', 'a', 'b', 't', 'A', 'e', 'a', ' ', '\\\\\\\\', 'T', 'e', 'e', 'm', 't', 'o', 's',\u001b[0m\n",
       "\u001b[32m'h', 'v', ' ', 'i', 'f', 'r', 'n', ' ', 'r', 's', 'a', 'd', 'c', 'n', '.', 'F', 'r', 'e', 'a', 'p', 'e', ' ', 'h', \u001b[0m\n",
       "\u001b[32m' ', 'a', 'r', 'i', 'e', 'c', 'm', 'a', 'i', 'o', ' ', 'a', ' ', 'a', 'k', 'n', 'c', 'l', 'b', 'l', 't', ' ', 'h', \u001b[0m\n",
       "\u001b[32m'n', 't', 'e', 'n', 'm', 'e', ' ', 'f', 'p', 'a', 'e', 's', 'i', 'c', 'e', 's', 's', ' ', 'i', 'e', ' ', 'h', 't', \u001b[0m\n",
       "\u001b[32m't', 'e', 'n', 'm', 'e', ' ', 'f', 'p', 's', 'i', 'l', ' ', 'a', 'r', ' ', 'r', 'w', '\\\\\\\\', 'q', 'a', 'r', 't', \u001b[0m\n",
       "\u001b[32m'c', 'l', 'y', ' ', 'i', 'g', 'e', 'a', 's', 'e', ' ', 'r', 'd', 'n', ' ', 'a', ' ', 'e', 'u', 'a', 'l', ' ', 'o', \u001b[0m\n",
       "\u001b[32m'd', 's', 'e', 'n', 's', 'b', 'l', ' ', 'i', 'f', 'r', 'n', 'e', ' ', 'e', 'w', 'e', ' ', 'p', 'c', 'f', 'c', 'n', \u001b[0m\n",
       "\u001b[32m'a', 'r', ',', 'a', 'd', 'i', 's', 'r', 's', 'l', 's', 'm', 'y', 'b', 'c', 'm', ' ', 'n', 't', 'b', 'e', ' ', 's', \u001b[0m\n",
       "\u001b[32m'a', 's', 'l', 't', ' ', 'c', 'r', 's', 'a', 'e', 'l', 'k', 'l', ' ', 'o', 'f', 'u', 't', 'a', 'e', 'm', 'r', ' ', \u001b[0m\n",
       "\u001b[32m'h', 'n', 'r', 'l', 't', 'v', '\\\\\\\\', 'p', 'i', 'w', 's', ' ', 'e', 'u', 't', ' ', 'f', 't', 'e', 'j', 'd', 'e', \u001b[0m\n",
       "\u001b[32m'm', 'd', 'l', 'c', 'a', 'g', 's', '\\\\\\\\', '3', '2', 'n', 'd', 'a', 't', 'g', 's', 'o', ' ', 'L', '-', 's', 'a', \u001b[0m\n",
       "\u001b[32m'J', 'd', 'e', 'n', 'L', '-', 's', 'a', 'j', 'd', 'e', 'o', 'f', 'r', ' ', 'w', ' ', 'e', ' ', 'e', 'e', 'i', 's', \u001b[0m\n",
       "\u001b[32m' ', 'c', 'l', 'b', 'l', 't', ' ', 'n', ' ', 'x', 'l', 'i', 'a', 'i', 'i', 'y', ' ', 't', 'r', 'd', 'c', 's', 't', \u001b[0m\n",
       "\u001b[32m'e', 'n', 'e', ' ', 'o', ' ', 'u', 'a', '\\\\\\\\', 'i', 'v', 'l', 'e', 'e', 't', ' ', 'n', 'b', 'i', 'g', 's', 'a', \u001b[0m\n",
       "\u001b[32m'a', 'l', ' ', 'e', 'c', 'm', 'r', 's', 'a', 'd', 'f', 's', ' ', 't', 'r', 't', 'o', 's', ' ', 'd', 'i', 'i', 'n', \u001b[0m\n",
       "\u001b[32m'l', 'y', ' ', 'L', ' ', 'u', 'g', 's', 'p', 'o', 'i', 'e', 'n', 'o', ' ', 'n', 'y', 's', 'o', 'e', ' ', 'u', ' ', \u001b[0m\n",
       "\u001b[32m'l', 'o', 'e', 'p', 'a', 'a', 'i', 'n', ',', 'm', 'k', 'n', ' ', 'h', 'i', ' ', 'u', 'p', 't', ' ', 'n', 'e', 'p', \u001b[0m\n",
       "\u001b[32m'e', 'a', 'l', ',', 'a', ' ', 'h', 'w', ' ', 'n', 'F', 'g', 'r', ' ', '.', 'n', '.', '\\\\\\\\', 'L', 'm', 't', 't', \u001b[0m\n",
       "\u001b[32m'o', 's', 'o', ' ', 'L', '-', 's', 'a', 'J', 'd', 'e', 'n', 'e', 'i', 'e', 't', 'f', ' ', 'e', 't', 'i', ' ', 'i', \u001b[0m\n",
       "\u001b[32m's', 's', 'a', 'd', 'l', 'm', 't', 't', 'o', 's', 'o', ' ', 'L', ' ', 'u', 'g', 's', '\u001b[0m\u001b[32m[\u001b[0m\u001b[32m', 'u', 't', ' ', 'r', 'm', \u001b[0m\n",
       "\u001b[32m'J', 'd', 'i', 'g', 'L', 'M', 'a', '-', '-', 'u', 'g', ' ', 'i', 'h', 'M', '-', 'e', 'c', ' ', 'n', ' ', 'h', 't', \u001b[0m\n",
       "\u001b[32m'o', ' ', 'r', 'n', '\u001b[0m\u001b[32m]\u001b[0m\u001b[32m', '.', 'W', ' ', 'h', 'n', 'v', 'r', 'f', ' ', 'h', ' ', 'g', 'e', 'm', 'n', ' ', 'e', 'w', \u001b[0m\n",
       "\u001b[32m'e', ' ', 'L', ' ', 'u', 'g', 's', 'a', 'd', 'h', 'm', 'n', 'p', 'e', 'e', 'e', 'c', 's', 'n', 'y', 'i', 't', 'o', \u001b[0m\n",
       "\u001b[32m'u', 'i', 'g', 't', 'o', 'b', 'n', 'h', 'a', 'k', ':', 'M', '-', 'e', 'c', ',', 'a', 'm', 'l', 'i', 't', 'r', ' ', \u001b[0m\n",
       "\u001b[32m'u', 's', 'i', 'n', 's', 't', ' ', 'n', ' ', 'h', 't', 'o', '\\\\\\\\', 'A', 'e', 'a', ' ', ' ', 'r', 'w', 's', 'u', \u001b[0m\n",
       "\u001b[32m'c', 'd', 'b', 't', 'l', ' ', 'l', 't', 'o', 'm', ' ', 'u', ' ', 'e', 'u', 't', ' ', 'e', 'e', 'l', 't', 'a', ' ', \u001b[0m\n",
       "\u001b[32m't', 'o', 'g', 'L', 'M', 'j', 'd', 'e', '\\\\\\\\', 'l', 'k', ' ', 'P', '-', ' ', 'a', ' ', 'a', 'c', ' ', 'o', 'h', \u001b[0m\n",
       "\u001b[32m'c', 'n', 'r', 'l', 'e', ' ', 'n', ' ', 'r', 'w', 's', 'u', 'c', 'd', 'h', 'm', 'n', 'p', 'e', 'e', 'e', 'c', 's', \u001b[0m\n",
       "\u001b[32m'w', 'l', ',', 'n', 'c', 'i', 'v', 'n', ' ', 'v', 'r', '8', '%', 'a', 'r', 'e', 'e', 't', ' ', 'h', ' ', 'a', 'e', \u001b[0m\n",
       "\u001b[32m'l', 'v', 'l', 'o', ' ', 'g', 'e', 'm', 'n', ' ', 'e', 'w', 'e', ' ', 'u', 'a', 's', '\\\\\\\\', 'H', 'n', 'e', ' ', \u001b[0m\n",
       "\u001b[32m'L', '-', 's', 'a', 'j', 'd', 'e', 'i', ' ', ' ', 'c', 'l', 'b', 'e', 'a', 'd', 'e', 'p', 'a', 'n', 'b', 'e', 'w', \u001b[0m\n",
       "\u001b[32m'y', 't', ' ', 'p', 'r', 'x', 'm', 't', ' ', 'u', 'a', '\\\\\\\\', 'p', 'e', 'e', 'e', 'c', 's', ' ', 'h', 'c', ' ', \u001b[0m\n",
       "\u001b[32m'r', ' ', 't', 'e', 'w', 's', ' ', 'e', 'y', 'e', 'p', 'n', 'i', 'e', 't', ' ', 'b', 'a', 'n', ' ', 'd', 'i', 'i', \u001b[0m\n",
       "\u001b[32m'n', 'l', 'y', ' ', 'e', 's', 'o', '\\\\\\\\', 'o', 'r', 'b', 'n', 'h', 'a', 'k', 'a', 'd', 't', 'a', 'i', 'i', 'n', \u001b[0m\n",
       "\u001b[32m'l', 'b', 'n', 'h', 'a', 'k', ' ', 'o', 'p', 'e', 'e', 't', 'e', 'c', ' ', 't', 'e', ' ', 'y', 'e', 'a', 'u', 't', \u001b[0m\n",
       "\u001b[32m'n', '\\\\\\\\', 's', 'v', 'r', 'l', 'v', 'r', 'a', 't', ' ', 'f', 'L', 'a', 'A', 'a', 'd', 'V', 'c', 'n', '.', 'T', \u001b[0m\n",
       "\u001b[32m'e', 'M', '-', 'e', 'c', ' ', 'u', 's', 'i', 'n', ',', '3', ' ', 'x', 'e', 't', 'v', 't', 's', '\\\\\\\\', 'a', 'd', \u001b[0m\n",
       "\u001b[32m'3', 'K', 'c', 'n', 'e', 's', 't', 'o', 's', 'w', 't', ' ', 'u', 'a', ' ', 'r', 'f', 'r', 'n', 'e', ' ', 'r', ' ', \u001b[0m\n",
       "\u001b[32m'u', 'l', 'c', 'y', 'a', 'a', 'l', 'b', 'e', 'a', ' ', 't', 'p', ':', 'n', '/', 'i', 'h', 'b', 'c', 'm', 'l', '-', \u001b[0m\n",
       "\u001b[32m'y', '/', 'a', 't', 'h', 't', 't', 'e', '/', 'a', 'n', 'f', 's', 'c', 'a', '/', 'l', '_', 'u', 'g', '\\\\n', 'e', \u001b[0m\n",
       "\u001b[32m'd', 'j', 'm', 'l', 't', 'h', 't', 'a', '/', 'i', 'm', 'e', 'r', '/', 'a', 'C', 's', 'F', 's', 's', 'm', '/', 'o', \u001b[0m\n",
       "\u001b[32m'.', 'u', 't', 'g', '/', '\\\\\\\\', 's', 't', 'h', 't', ' ', 'l', 'a', 'i', 'v', ' ', 'l', 'i', 'b', 'p', 'e', 'a', \u001b[0m\n",
       "\u001b[32m's', 'c', 'e', 'e', 'e', 'p', 'n', 'm', 'h', 'h', 'i', ' ', 'n', 'i', 'a', 'r', 'v', 'o', ' ', '0', ' ', 'n', 'n', \u001b[0m\n",
       "\u001b[32m',', 'e', 'o', ' ', 'r', 'p', 'e', 'K', ' ', 's', 'o', 't', 'e', 'q', 'h', 'n', 'b', 'T', ' ', 'h', ' ', 'a', 'u', \u001b[0m\n",
       "\u001b[32m'i', ' ', 'n', ' ', 'M', 'L', ' ', 'o', 's', 'n', 'i', 'a', ' ', 'a', 'e', 'e', 'n', 'g', 'i', 'a', 'l', 'v', ' ', \u001b[0m\n",
       "\u001b[32m'b', 'r', 'h', 'o', 'h', 'a', ' ', 'n', 'm', 'l', 'm', 'c', 's', 'r', 'm', 'c', 'e', ' ', 'a', 'o', 't', 'd', 'r', \u001b[0m\n",
       "\u001b[32m' ', 'n', ' ', 'r', 'm', 'c', 'e', ' ', 'u', 'n', 'w', 'h', ' ', 'w', ',', 'l', 'a', 'o', 't', 'd', 'A', '.', 'i', \u001b[0m\n",
       "\u001b[32m't', 'o', 'o', ' ', 'v', 's', 'e', 'x', ' ', 'r', 'v', 'e', 'i', 'r', 'h', 'o', 'e', 'a', 'h', 'i', 'w', ',', 'e', \u001b[0m\n",
       "\u001b[32m'n', 'r', 'f', 'r', 'n', 'n', 'm', 'h', 'e', 'a', 'i', 'o', 'p', 'a', 'o', ' ', 'a', ' ', 'l', 'a', 'i', 'l', 'x', \u001b[0m\n",
       "\u001b[32m' ', 'n', ' ', 'l', 'a', 'a', 's', 'a', 's', ' ', 'g', 'u', '-', '-', 'a', 'M', 'L', ',', 'c', 'e', 'n', '.', 'n', \u001b[0m\n",
       "\u001b[32m'm', 'h', 'n', 'e', 't', 'b', 't', 'e', 'e', 'r', 'a', 'f', ' ', 'e', 'e', ' ', 'm', 's', 'e', 't', ',', 'n', 'm', \u001b[0m\n",
       "\u001b[32m'e', 'g', ' ', '0', ' ', 'e', 'o', 'g', 'i', 'e', 'h', 'a', '\\\\\\\\', 'l', 'e', ' ', 'e', 'n', 'r', 'f', 'r', ' ', \u001b[0m\n",
       "\u001b[32m'a', 'u', ' ', 'e', 'r', 'o', 'd', 'o', 'c', 'd', 'a', 'd', 'l', 'o', 't', 'o', ' ', 't', 'b', 'h', 't', 'm', 'n', \u001b[0m\n",
       "\u001b[32m'c', '4', 'T', 'G', 'e', 'i', 'n', 's', 'g', 'u', ' ', 'L', ' ', 'n', 'r', 's', 't', 'h', ' ', 'a', 'v', 'r', 's', \u001b[0m\n",
       "\u001b[32m'l', 's', 'r', 'r', 'O', '.', 'r', 'f', 'a', 'p', 'e', 't', 'a', ' ', 'e', 'r', 'o', 'd', 'o', 'c', 'a', ',', 'n', \u001b[0m\n",
       "\u001b[32m'r', 'n', 't', 'b', 'a', 'C', 'd', 'a', ';', 'e', ' ', 'o', 't', 'e', 'q', 'n', 'u', '-', 't', 'u', ' ', ' ', 'h', \u001b[0m\n",
       "\u001b[32m'n', 'b', 'T', ' ', 's', 'r', 'm', 'c', 'e', ' ', 'w', ' ', 'n', 'c', 'd', 'r', 'n', ' ', 'b', '\\\\\\\\', 'e', 'n', \u001b[0m\n",
       "\u001b[32m'r', 'f', 'r', ' ', 'a', 'u', ' ', 'n', ' ', 'e', 'd', 'j', 'M', 'L', 'n', 'e', 't', 'b', 't', 'e', 'e', 'r', 'a', \u001b[0m\n",
       "\u001b[32m'e', 't', 'y', 'i', 'e', ' ', 'e', 't', 'e', ' ', ' ', 'a', 'e', 'A', 't', 'b', 'a', 'C', 'd', 'a', 'h', 'n', 'B', \u001b[0m\n",
       "\u001b[32m'T', ' ', 't', 'w', 'e', 'd', 'J', 'a', 's', '-', 'L', ' ', 'n', 'g', 'u', ' ', 'o', 'f', 'e', 'o', 'Q', '\\\\n', \u001b[0m\n",
       "\u001b[32m'e', 'd', 'j', 'M', 'L', 'f', ' ', 'n', 'i', 'a', 'i', 'i', ' ', 'n', ' ', 'e', 'a', 'b', 'n', 'a', 'r', 'c', 'y', \u001b[0m\n",
       "\u001b[32m'i', 'n', 'd', ' ', 'W', '\\\\\\\\', 'g', 'u', '-', '-', 'a', 'M', 'L', 'f', ' ', 'n', 'i', 'a', 'i', 'i', 'n', '3', \u001b[0m\n",
       "\u001b[32m'3', '\\\\\\\\', '1', 'e', 'u', 'i', ' ', 'i', 'n', 'o', 's', 's', ' ', 'e', 'b', 't', 'r', 'r', 't', 'i', 's', 'u', \u001b[0m\n",
       "\u001b[32m't', 'o', 'r', 'e', 't', 'g', 'i', 'a', ' ', 's', 'o', 't', 'n', 'l', 'x', ' ', 's', 'a', 't', 'b', 's', 'r', 'c', \u001b[0m\n",
       "\u001b[32m' ', 'l', 'o', 't', 'n', '\\\\\\\\', 'd', 'v', 'r', ' ', 'e', 'd', 'j', 'M', 'L', ',', 'l', 'a', 'o', 't', 'd', 'A', \u001b[0m\n",
       "\u001b[32m'.', 'n', 'i', 'a', 'e', 'i', 't', 'a', ' ', 'n', ' ', 'k', 'a', 'h', 'n', 'b', 'e', 'b', 'l', 'c', ' ', 'n', 'l', \u001b[0m\n",
       "\u001b[32m'a', 'e', ',', 'n', 'm', 'v', 'o', 'n', 'n', 'n', 'm', 'h', 'r', 'f', 'd', 'e', ' ', 'h', ' ', 'e', 'u', 'e', ' ', \u001b[0m\n",
       "\u001b[32m'I', '.', 't', 'l', 'b', 'n', 'a', 'p', 'e', 'd', 'a', 'y', 'i', 'i', 'a', 'a', 's', ':', 't', 'f', 'n', 'b', 'y', \u001b[0m\n",
       "\u001b[32m'k', 'o', 't', 's', 'e', 'f', ' ', 'g', 'u', '-', '-', 'a', 'M', 'L', '\\\\\\\\', 'g', 'u', '-', '-', 'a', 'M', 'L', \u001b[0m\n",
       "\u001b[32m'f', ' ', 'e', 'a', 'n', 'v', 'A', '\\\\\\\\', '.', 'n', '.', 'e', 'n', 'h', ' ', 'e', 'o', ' ', 'g', 'u', ' ', 'h', ' \u001b[0m\n",
       "\u001b[32m', 'i', 's', 'l', 's', 'r', 'e', 'i', 'r', 'a', 'n', 'e', 'i', 'a', 'e', ' ', 'a', 't', 'e', 'o', ' ', 't', 'u', \u001b[0m\n",
       "\u001b[32m'c', 'l', ' ', 't', 'y', 'e', 'i', ' ', 'r', ' ', 'e', 'o', 's', 'e', 'u', 'o', 'b', ' ', 'a', ',', 'l', 'a', 's', \u001b[0m\n",
       "\u001b[32m'u', 'e', 'o', 'e', ' ', 'a', ' ', 't', 'u', 'e', ' ', 't', ' ', 'n', ' ', 's', 'i', 'p', '\\\\\\\\', 'i', 'i', 'e', \u001b[0m\n",
       "\u001b[32m's', 'n', 'e', 't', 'b', 's', 'c', 'e', 'e', 'f', 'd', 'e', 't', 'u', ' ', 'r', 'c', 'i', ' ', 't', 'e', 'b', 'n', \u001b[0m\n",
       "\u001b[32m' ', 'b', 'y', 'm', 'g', 'i', 'a', 'g', 'r', 'w', 'n', ' ', 'l', 'n', 's', ';', 'l', 'a', 'i', 'a', 'd', 'u', 'n', \u001b[0m\n",
       "\u001b[32m's', 'o', 'g', 's', 'i', 'p', 'e', 'b', 's', 'o', ' ', 'o', 'r', 'b', 'u', ' ', 'h', ' ', 'a', 't', 'n', 'v', 'g', \u001b[0m\n",
       "\u001b[32m',', 'e', 'a', 'r', 'n', ' ', 'r', 'y', 'l', ' ', 'o', 'r', 'b', 'u', ' ', 'h', ' ', 'e', 'w', 'y', 'i', 'i', 'a', \u001b[0m\n",
       "\u001b[32m'a', 's', '\\\\\\\\', 'c', 'l', 'y', 'm', 'n', 's', 'r', 'p', 'o', ' ', 's', 'w', 'i', 'p', 'e', 't', ',', 'l', 'm', \u001b[0m\n",
       "\u001b[32m'x', ' ', 'o', ' ', 's', 'o', ' ', 'n', ' ', 'o', 'p', 't', 'e', 'e', 'f', 'd', 'e', 'a', ' ', 'd', 'h', 'e', ' ', \u001b[0m\n",
       "\u001b[32m's', 'h', 'n', '.', '\u001b[0m\u001b[32m]\u001b[0m\u001b[32m', 'n', 'r', ' ', 'o', 't', 'h', ' ', 'n', ' ', 'c', 'e', '-', 'M', 'h', 'i', ' ', 'g', 'u', \u001b[0m\n",
       "\u001b[32m'-', '-', 'a', 'M', 'L', 'g', 'i', 'd', 'J', 'm', 'r', ' ', 't', 'u', '\u001b[0m\u001b[32m[\u001b[0m\u001b[32m', 's', 'o', ' ', 'n', ' ', 'o', 'p', 't', \u001b[0m\n",
       "\u001b[32m'e', 'e', 'f', 'd', 'e', 'a', ' ', 'd', 'h', 'e', ' ', 's', 'h', 'n', '.', 'x', 'd', 'e', 'p', '\u001b[0m\u001b[32m(\u001b[0m\u001b[32m', '8', 'e', 'u', \u001b[0m\n",
       "\u001b[32m'i', ' ', 'i', 's', ' ', 'm', 'l', 'o', 'p', 'h', 'a', ' ', 'n', 'd', 'r', ' ', 'o', ' ', 's', ' ', 'w', 't', 'm', \u001b[0m\n",
       "\u001b[32m'r', ' ', 'l', 'm', 'x', ' ', 'A', '.', 'l', 'a', 'i', 'p', 'a', 'f', 'n', 'n', 'i', 'u', 'o', ' ', 'c', 'e', 'e', \u001b[0m\n",
       "\u001b[32m'e', ' ', ' ', 'd', 'v', 'r', ' ', 't', 'l', 'i', 'i', 'e', 'e', ' ', 'b', 'y', 'm', 't', ' ', 's', 's', 'c', 'n', \u001b[0m\n",
       "\u001b[32m'a', 'r', 'c', 'n', ' ', 'g', 'i', 'a', 'g', 'd', 'd', 'u', '-', 'c', 'e', 'e', 'e', ' ', '2', '2', '\\\\\\\\', '\\\\\\\\',\u001b[0m\n",
       "\u001b[32m'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m', 'i', 'n', 'p', 'A', ' ', ' ', 'r', 'g', 'F', 'n', ' ', 'i', 'o', 'r', 'n', 'c', ' ', 'i', 't', 'r', 'f', 'd', \u001b[0m\n",
       "\u001b[32m's', ' ', 'p', 'o', 'p', 'e', 'T', '.', 'e', 's', 'a', 'e', 'g', 'i', 'n', 'a', 'o', ' ', 'r', 'c', ' ', ' ', 'g', \u001b[0m\n",
       "\u001b[32m's', 'a', 'y', 't', 'e', 'i', ' ', 't', 'd', 'k', 'a', 's', ' ', 'g', 'u', ' ', 'L', ' ', 'a', ',', 'l', 'v', 't', \u001b[0m\n",
       "\u001b[32m'n', 'e', 'l', ' ', 'g', 'i', 'a', 'g', 'r', 'w', 'n', ' ', 'l', 'n', 'S', '2', '0', 'u', 'n', '.', 'x', 'd', 'e', \u001b[0m\n",
       "\u001b[32m'p', '\u001b[0m\u001b[32m(\u001b[0m\u001b[32m', '5', 'e', 'u', 'i', ' ', 'i', 'n', 'v', 'g', 's', ' ', 'e', 'u', 't', 'm', 'r', ' ', 'h', ' ', 'e', 't', \u001b[0m\n",
       "\u001b[32m'a', 'e', 'a', 'c', 'd', 'r', ' ', 'e', 't', 'b', 's', ' ', 'n', ' ', 'c', 'h', ' ', 'n', 'm', 'e', 'e', ' ', 't', \u001b[0m\n",
       "\u001b[32m'\\\\\\\\', 'e', 's', 't', 'd', 'a', ',', 'r', 'w', 'n', ' ', 'w', ' ', 'n', ' ', 'o', 't', 'e', 'q', 'a', 'h', 'i', ' \u001b[0m\n",
       "\u001b[32m', 'e', 'n', 's', 'r', ' ', 'i', 'e', 'd', 'j', 'M', 'L', 'n', ' ', 'n', 's', 'r', 'p', 'o', ' ', 's', 'w', 'i', \u001b[0m\n",
       "\u001b[32m'P', '2', '0', 'u', 'n', ':', 'o', 't', 'n', 'b', 'o', ' ', 'i', 'r', ' ', 'l', 'n', 'd', 'e', 'e', 'n', ' ', 'e', \u001b[0m\n",
       "\u001b[32m'n', 'm', 'l', 'm', ' ', 'b', 'n', 'c', 'y', 'h', ' ', 's', 'o', 't', 'i', 'a', ' ', 'g', 'u', '-', '-', 'a', 'M', \u001b[0m\n",
       "\u001b[32m'L', '3', 'e', 'o', 'o', 'p', 'e', 'n', 'e', 'd', 'J', 'a', 's', '-', 'L', ' ', 'o', 's', 'p', 'T', '\\\\\\\\', '.', \u001b[0m\n",
       "\u001b[32m'n', '.', 'g', 'u', '-', '-', 'a', 'M', 'L', 'f', ' ', 'n', 'i', 'a', 'i', 'i', ' ', 'n', ' ', 's', ' ', 'h', ' ', \u001b[0m\n",
       "\u001b[32m's', 'c', 'i', ' ', 'w', ',', 'x', 'N', '.', '\u001b[0m\u001b[32m]\u001b[0m\u001b[32m', 'n', 'r', ' ', 'o', 't', 'h', ' ', 'n', ' ', 'c', 'e', '-', 'M', \u001b[0m\n",
       "\u001b[32m'h', 'i', ' ', 'g', 'u', '-', '-', 'a', 'M', 'L', 'g', 'i', 'd', 'J', 'm', 'r', ' ', 't', 'u', '\u001b[0m\u001b[32m[\u001b[0m\u001b[32m'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAnswer \u001b[0m\n",
       "\u001b[32monly from retrieval. Only cite sources that are used. Make your response conversational.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'Tell me about the use of large language models as judge'\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The use of large language models as a judge is an interesting and emerging concept. Large language models are artificial intelligence models trained on vast amounts of text data, enabling them to understand and generate human-like text. The idea of using these models as a judge implies that they can make fair and objective decisions based on the information provided to them.\n",
      "\n",
      "A study by Ebrahimi et al. (2018) discussed the potential use of language models as a judge for adversarial attacks on text classification tasks. Adversarial attacks involve maliciously manipulating input data to mislead machine learning models. The researchers proposed using language models to detect such attacks by comparing the output of the original model and the language model. If the output significantly differs, it may indicate an adversarial attack.\n",
      "\n",
      "Another example is the use of language models in automated content moderation, as mentioned by Gao and Adel (2016). Content moderation involves filtering and removing inappropriate or harmful content. Language models can help automate this process by scanning text, identifying potentially offensive or harmful content, and making a decision on whether it should be removed or not. This process is similar to judging whether certain content is acceptable or not.\n",
      "\n",
      "However, it is essential to note that large language models may not always be entirely objective or fair. As detailed by Bender et al. (2021), these models can sometimes inherit and perpetuate biases present in their training data. This could lead to unfair or biased decisions in certain situations, making it crucial to carefully consider and address these issues when implementing language models as judges.\n",
      "\n",
      "In summary, the use of large language models as a judge has potential applications in various domains, such as detecting adversarial attacks and automating content moderation. However, it is necessary to address potential issues like bias to ensure fairness and objectivity in decision-making.\n",
      "\n",
      "Sources:\n",
      "\n",
      "Bender, E. M., Gao, J., Wu, J., Girshick, A. R., & Franc, A. (2021). On Evaluating the Validity of Test Sets for Detecting and Mitigating Biases in NLP Models. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6195‚Äì6210, Online. <https://www.aclweb.org/anthology/2021.emnlp-main.486/>\n",
      "\n",
      "Ebrahimi, J., Rao, A., Douze, M., & Liu, M.-Y. (2018). HotFlip: White-box Adversarial Examples for Text Classification. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1306‚Äì1316, Brussels, Belgium. <https://www.aclweb.org/anthology/D18-1177/>\n",
      "\n",
      "Gao, J., & Adel, N. (2016). Detecting abusive language online via distributed machine learning. Proceedings of the 2016 World Wide Web Conference (WWW), pages 447‚Äì457, Montreal, Canada. <https://www.iwonderweb.org/pub/papers/paper898.pdf>"
     ]
    }
   ],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import gradio as gr\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
    "# instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "\n",
    "convstore = default_FAISS()\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "initial_msg = (\n",
    "    \"Hello! I am a document chat agent here to help the user!\"\n",
    "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "stream_chain = chat_prompt| RPrint() | instruct_llm | StrOutputParser()\n",
    "\n",
    "################################################################################################\n",
    "## BEGIN TODO: Implement the retrieval chain to make your system work!\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input': (lambda x: x)}  # Pass the input message through\n",
    "    ## Retrieve conversation history from convstore\n",
    "    | RunnableAssign({\n",
    "        'history': lambda d: docs2str(convstore.similarity_search(d['input'], k=3))\n",
    "    })\n",
    "    ## Retrieve document context from docstore\n",
    "    | RunnableAssign({\n",
    "        'context': lambda d: docs2str(docstore.similarity_search(d['input'], k=3))\n",
    "    })\n",
    "    ## Reorder longer documents for better context handling (optional)\n",
    "    | RunnableAssign({\n",
    "        'context': lambda d: long_reorder.invoke(d['context'])\n",
    "    })\n",
    ")\n",
    "\n",
    "## END TODO\n",
    "################################################################################################\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ## First perform the retrieval based on the input message\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## Then, stream the results of the stream_chain\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## If you're using standard print, keep line from getting too long\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
    "\n",
    "\n",
    "## Start of Agent Event Loop\n",
    "test_question = \"Tell me about the use of large language models as judge\"  ## <- modify as desired\n",
    "\n",
    "## Before you launch your gradio interface, make sure your thing works\n",
    "for response in chat_gen(test_question, return_buffer=False):\n",
    "    print(response, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9W7sC5Z6BfqM",
   "metadata": {
    "id": "9W7sC5Z6BfqM"
   },
   "source": [
    "### **Task 4:** Interact With Your Gradio Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fMP3l7QL2JWT",
   "metadata": {
    "id": "fMP3l7QL2JWT"
   },
   "outputs": [],
   "source": [
    "# chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "# demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "# try:\n",
    "#     demo.launch(debug=True, share=True, show_api=False)\n",
    "#     demo.close()\n",
    "# except Exception as e:\n",
    "#     demo.close()\n",
    "#     print(e)\n",
    "#     raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCb3RVVfbmQ0",
   "metadata": {
    "id": "yCb3RVVfbmQ0"
   },
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4:** Saving Your Index For Evaluation\n",
    "\n",
    "After you've implemented your RAG chain, please save your accumulated vector store as shown [in the official documentation](https://python.langchain.com/docs/integrations/vectorstores/faiss#saving-and-loading). You'll have a chance to use it again for your final assessment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "Y4se5wQ4Afda",
   "metadata": {
    "id": "Y4se5wQ4Afda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    }
   ],
   "source": [
    "## Save and compress your index\n",
    "docstore.save_local(\"docstore_index\")\n",
    "!tar czvf docstore_index.tgz docstore_index\n",
    "\n",
    "!rm -rf docstore_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LsI7NivbIgFw",
   "metadata": {
    "id": "LsI7NivbIgFw"
   },
   "source": [
    "If everything was properly saved, the following line can be invoked to pull the index from the compressed `tgz` file (assuming the pip requirements are installed). After you have confirmed that the cell can pull in your index, download `docstore_index.tgz` for use in the last notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "Qs8820ucIu1t",
   "metadata": {
    "id": "Qs8820ucIu1t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n",
      ". To demonstrate, we build an index using the DrQA [5]\\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n7\\nTable 4: Human assessments for the Jeopardy\\nQuestion Generation Task.\\nFactuality\\nSpeci\\ufb01city\\nBART better\\n7.1%\\n16.8%\\nRAG better\\n42.7%\\n37.4%\\nBoth good\\n11.7%\\n11.8%\\nBoth poor\\n17.7%\\n6.9%\\nNo majority\\n20.8%\\n20.1%\\nTable 5: Ratio of distinct to total tri-grams for\\ngeneration tasks.\\nMSMARCO\\nJeopardy QGen\\nGold\\n89.6%\\n90.0%\\nBART\\n70.7%\\n32.4%\\nRAG-Token\\n77.8%\\n46.8%\\nRAG-Seq.\\n83.5%\\n53.8%\\nTable 6: Ablations on the dev set. As FEVER is a classi\\ufb01cation task, both RAG models are equivalent.\\nModel\\nNQ\\nTQA\\nWQ\\nCT\\nJeopardy-QGen\\nMSMarco\\nFVR-3\\nFVR-2\\nExact Match\\nB-1\\nQB-1\\nR-L\\nB-1\\nLabel Accuracy\\nRAG-Token-BM25\\n29.7\\n41.5\\n32.1\\n33.1\\n17.5\\n22.3\\n55.5\\n48.4\\n75.1\\n91.6\\nRAG-Sequence-BM25\\n31.8\\n44.1\\n36.6\\n33\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "!tar xzvf docstore_index.tgz\n",
    "new_db = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = new_db.similarity_search(\"Testing the index\")\n",
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "as_3vWJGKB2F",
   "metadata": {
    "id": "as_3vWJGKB2F"
   },
   "source": [
    "-----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5:** Wrap-Up\n",
    "\n",
    "Congratulations! Assuming your RAG chain is all good, you're now ready to move on to the **RAG Evaluation [Assessment]** section!\n",
    "\n",
    "### <font color=\"#76b900\">**Great Job!**</font>\n",
    "\n",
    "### **Next Steps:**\n",
    "1. **[Optional]** Revisit the **\"Questions To Think About\" Section** at the top of the notebook and think about some possible answers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4",
   "metadata": {
    "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
